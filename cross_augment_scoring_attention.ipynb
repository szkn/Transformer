{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyRQwYrEYTpv"
   },
   "source": [
    "# 修士論文実験 2022/01/25\n",
    "## cross_validationのコード実装\n",
    "## 正解データはスコアの得点\n",
    "## データの水増し実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 843,
     "status": "ok",
     "timestamp": 1636987406075,
     "user": {
      "displayName": "Kento Suzuki",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16345288529127918743"
     },
     "user_tz": -540
    },
    "id": "dO5GxI7RzXIc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import Subset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from pytorchtools import EarlyStopping\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# sys.path.append(os.path.abspath(\"..\"))\n",
    "# import scripts.compensate\n",
    "# import scripts.kinectImg2video\n",
    "# from scripts.conpemsate_suppresser import *\n",
    "# from scripts.ground_angle_analysis import ground_shoulder_angle_analyzer\n",
    "# import scripts.ground_angle_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup seeds\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日付の追加\n",
    "df_annotate = pd.read_excel(\"annotate_dataset.xlsx\")\n",
    "df_annotate['date'] = pd.to_datetime(df_annotate['date'], format='%Y-%m-%d-%H-%M-%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>uid</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>task_id</th>\n",
       "      <th>point</th>\n",
       "      <th>shoulder</th>\n",
       "      <th>body</th>\n",
       "      <th>flag_usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-06 18:56:20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-12-06 18:56:20</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-12-06 18:56:41</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-12-06 18:56:41</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-06 18:56:58</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date  uid  subject_id  task_id point  shoulder  body  \\\n",
       "0 2021-12-06 18:56:20    1           1        1     3         0     0   \n",
       "1 2021-12-06 18:56:20    2           1        1     4         0     0   \n",
       "2 2021-12-06 18:56:41    3           1        1     3         0     0   \n",
       "3 2021-12-06 18:56:41    4           1        1     4         0     0   \n",
       "4 2021-12-06 18:56:58    5           1        1     3         0     0   \n",
       "\n",
       "   flag_usage  \n",
       "0           1  \n",
       "1           2  \n",
       "2           1  \n",
       "3           2  \n",
       "4           1  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annotate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>id</th>\n",
       "      <th>x_Pelvis</th>\n",
       "      <th>y_Pelvis</th>\n",
       "      <th>z_Pelvis</th>\n",
       "      <th>x_SpineNaval</th>\n",
       "      <th>y_SpineNaval</th>\n",
       "      <th>z_SpineNaval</th>\n",
       "      <th>x_SpineChest</th>\n",
       "      <th>...</th>\n",
       "      <th>x_REar</th>\n",
       "      <th>y_REar</th>\n",
       "      <th>z_REar</th>\n",
       "      <th>N</th>\n",
       "      <th>v_RWrist</th>\n",
       "      <th>z_v_RWrist</th>\n",
       "      <th>flag_active</th>\n",
       "      <th>flag_moving</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-158.973129</td>\n",
       "      <td>353.261841</td>\n",
       "      <td>698.233093</td>\n",
       "      <td>-149.886017</td>\n",
       "      <td>218.100922</td>\n",
       "      <td>780.778748</td>\n",
       "      <td>-142.035919</td>\n",
       "      <td>...</td>\n",
       "      <td>-178.933960</td>\n",
       "      <td>-236.272278</td>\n",
       "      <td>842.268127</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.917988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2021-12-04 18:20:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-156.911919</td>\n",
       "      <td>356.031863</td>\n",
       "      <td>697.378637</td>\n",
       "      <td>-147.957493</td>\n",
       "      <td>219.063530</td>\n",
       "      <td>781.238292</td>\n",
       "      <td>-142.348493</td>\n",
       "      <td>...</td>\n",
       "      <td>-176.788628</td>\n",
       "      <td>-241.853940</td>\n",
       "      <td>862.009244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.131710</td>\n",
       "      <td>0.480634</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.891892</td>\n",
       "      <td>2021-12-04 18:20:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-157.406120</td>\n",
       "      <td>356.970863</td>\n",
       "      <td>697.867036</td>\n",
       "      <td>-148.184343</td>\n",
       "      <td>219.214968</td>\n",
       "      <td>781.799317</td>\n",
       "      <td>-142.899719</td>\n",
       "      <td>...</td>\n",
       "      <td>-176.093690</td>\n",
       "      <td>-243.703880</td>\n",
       "      <td>867.831643</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.690953</td>\n",
       "      <td>0.198082</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79.783784</td>\n",
       "      <td>2021-12-04 18:20:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-159.493486</td>\n",
       "      <td>356.687710</td>\n",
       "      <td>699.214911</td>\n",
       "      <td>-149.753131</td>\n",
       "      <td>218.849632</td>\n",
       "      <td>782.412353</td>\n",
       "      <td>-143.553739</td>\n",
       "      <td>...</td>\n",
       "      <td>-176.457427</td>\n",
       "      <td>-243.071735</td>\n",
       "      <td>864.096621</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.280899</td>\n",
       "      <td>0.313779</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>119.675676</td>\n",
       "      <td>2021-12-04 18:20:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-162.211771</td>\n",
       "      <td>355.791270</td>\n",
       "      <td>700.938883</td>\n",
       "      <td>-151.850422</td>\n",
       "      <td>218.261918</td>\n",
       "      <td>783.027933</td>\n",
       "      <td>-144.174692</td>\n",
       "      <td>...</td>\n",
       "      <td>-177.488121</td>\n",
       "      <td>-241.207140</td>\n",
       "      <td>855.165480</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.247976</td>\n",
       "      <td>0.307322</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>159.567568</td>\n",
       "      <td>2021-12-04 18:20:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1   id    x_Pelvis    y_Pelvis    z_Pelvis  \\\n",
       "0           0             0  1.0 -158.973129  353.261841  698.233093   \n",
       "1           1             1  1.0 -156.911919  356.031863  697.378637   \n",
       "2           2             2  1.0 -157.406120  356.970863  697.867036   \n",
       "3           3             3  1.0 -159.493486  356.687710  699.214911   \n",
       "4           4             4  1.0 -162.211771  355.791270  700.938883   \n",
       "\n",
       "   x_SpineNaval  y_SpineNaval  z_SpineNaval  x_SpineChest  ...      x_REar  \\\n",
       "0   -149.886017    218.100922    780.778748   -142.035919  ... -178.933960   \n",
       "1   -147.957493    219.063530    781.238292   -142.348493  ... -176.788628   \n",
       "2   -148.184343    219.214968    781.799317   -142.899719  ... -176.093690   \n",
       "3   -149.753131    218.849632    782.412353   -143.553739  ... -176.457427   \n",
       "4   -151.850422    218.261918    783.027933   -144.174692  ... -177.488121   \n",
       "\n",
       "       y_REar      z_REar   N  v_RWrist  z_v_RWrist  flag_active  flag_moving  \\\n",
       "0 -236.272278  842.268127 NaN       NaN   -0.917988          0.0          1.0   \n",
       "1 -241.853940  862.009244 NaN  7.131710    0.480634          1.0          1.0   \n",
       "2 -243.703880  867.831643 NaN  5.690953    0.198082          1.0          1.0   \n",
       "3 -243.071735  864.096621 NaN  6.280899    0.313779          1.0          1.0   \n",
       "4 -241.207140  855.165480 NaN  6.247976    0.307322          1.0          1.0   \n",
       "\n",
       "    timestamp                date  \n",
       "0    0.000000 2021-12-04 18:20:18  \n",
       "1   39.891892 2021-12-04 18:20:18  \n",
       "2   79.783784 2021-12-04 18:20:18  \n",
       "3  119.675676 2021-12-04 18:20:18  \n",
       "4  159.567568 2021-12-04 18:20:18  \n",
       "\n",
       "[5 rows x 106 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataSetの呼び出し\n",
    "df_data = pd.read_csv(\"bigdata_trimmed.csv\")\n",
    "df_data['date'] = pd.to_datetime(df_data['date'], format='%Y-%m-%d-%H-%M-%S')\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1636987406076,
     "user": {
      "displayName": "Kento Suzuki",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16345288529127918743"
     },
     "user_tz": -540
    },
    "id": "GZlVgJy1zXIc"
   },
   "outputs": [],
   "source": [
    "# pd_merged = pd.merge(df_annotate, df_data, on='date', how='inner')\n",
    "# pd_3 = pd_merged[pd_merged['point'] == 3]\n",
    "# pd_2A = pd_merged[pd_merged['point'] == '2A']\n",
    "# pd_2B = pd_merged[pd_merged['point'] == '2B']\n",
    "# pd_2C = pd_merged[pd_merged['point'] == '2C']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### このデータセットはGraspのものかつ，usageが1のものが取り出されている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "# データセットにNN入力用のスコアを割り振る．\n",
    "# これが正解ラベルになる\n",
    "dataset = pd.merge(df_annotate, df_data, on='date', how='inner')\n",
    "\n",
    "dataset = dataset[(dataset['task_id'] == 1) & (dataset['flag_usage'] == 1)]\n",
    "dataset['point'] = dataset['point'].astype(str)\n",
    "dataset.loc[dataset['point'] == '3', 'score'] = 3\n",
    "dataset.loc[dataset['point'] == '2A', 'score'] = 2\n",
    "dataset.loc[dataset['point'] == '2B', 'score'] = 1\n",
    "dataset.loc[dataset['point'] == '2C', 'score'] = 0\n",
    "print(dataset['score'].max())\n",
    "\n",
    "# Nanのものを削除\n",
    "dataset.dropna(subset=['score'], inplace=True)\n",
    "# scoreの行をintにする\n",
    "dataset['score'] = dataset['score'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  3,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 31, 32, 33, 34, 35,\n",
       "       36, 37, 38, 39, 40, 41, 42, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['uid'].unique()\n",
    "# dataset['date'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grasp動作だけを抜き出す\n",
    "# dataset_grasp = dataset[dataset['task_id'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RElbow2RWrist と Rshoulder2RElbow と Pelvis2RShoulder と Pelvis2Neck の速度を入れてみる "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_annotate\n",
    "# l_date = df_annotate['date'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_vector(df, joint1, joint2, word=\"\"):\n",
    "    df[f\"{word}_x_{joint1}2{joint2}\"] = df[f\"x_{joint1}\"].astype('float64') - df[f\"x_{joint2}\"].astype('float64')\n",
    "    df[f\"{word}_y_{joint1}2{joint2}\"] = df[f\"y_{joint1}\"].astype('float64') - df[f\"y_{joint2}\"].astype('float64')\n",
    "    df[f\"{word}_z_{joint1}2{joint2}\"] = df[f\"z_{joint1}\"].astype('float64') - df[f\"z_{joint2}\"].astype('float64')\n",
    "    df[f\"{word}_ave_{joint1}2{joint2}\"] = (df[f\"{word}_x_{joint1}2{joint2}\"] + df[f\"{word}_y_{joint1}2{joint2}\"] + df[f\"{word}_z_{joint1}2{joint2}\"])/3\n",
    "    return df\n",
    "\n",
    "def calc_angle(df, joint1, joint2, joint3):\n",
    "    l_theta = []\n",
    "    # 角度を算出する関数。p2を支点として、他の二点間の角度を算出する。\n",
    "    x1, y1 ,z1= df[f\"x_{joint1}\"].astype('float64'), df[f\"y_{joint1}\"].astype('float64'), df[f\"z_{joint1}\"].astype('float64')\n",
    "    x2, y2 ,z2= df[f\"x_{joint2}\"].astype('float64'), df[f\"y_{joint2}\"].astype('float64'), df[f\"z_{joint2}\"].astype('float64')\n",
    "    x3, y3 ,z3= df[f\"x_{joint3}\"].astype('float64'), df[f\"y_{joint3}\"].astype('float64'), df[f\"z_{joint3}\"].astype('float64')\n",
    "    v1 = np.array([x1-x2, y1-y2, z1-z2])\n",
    "    v2 = np.array([x3-x2, y3-y2, z3-z2])\n",
    "    for i in range(v1.shape[1]):\n",
    "        cos = np.dot(v1[:,i], v2[:,i])/(np.linalg.norm(v1[:,i], ord=2) * np.linalg.norm(v2[:,i], ord=2))\n",
    "        theta = np.degrees(math.acos(cos))\n",
    "        l_theta.append(theta)\n",
    "    return np.array(l_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 角度と角度の速度を入力とする\n",
    "angle_RElbow = calc_angle(dataset, 'RShoulder', 'RElbow', 'RWrist')\n",
    "diff_angle_RElbow = np.diff(angle_RElbow)\n",
    "\n",
    "angle_RShoulder = calc_angle(dataset, 'RElbow', 'RShoulder', 'Neck')\n",
    "diff_angle_RShoulder = np.diff(angle_RShoulder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l_date_name = np.array(dataset['date'].unique())\n",
    "# l = []\n",
    "# for date in l_date_name:\n",
    "#     score = dataset[dataset[\"date\"] == date]['score'].mode()\n",
    "#     l.append(score)\n",
    "# return np.array(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正解ラベルで用いるyのone hotベクトルを作成\n",
    "def extract_y(df, augment_flag=False, rate_augment=10):\n",
    "    l = []\n",
    "    l_date_name = np.array(df['date'].unique())\n",
    "    for date in l_date_name:\n",
    "        score = df[df[\"date\"] == date]['score'].mode()\n",
    "        l.append(score)\n",
    "        if augment_flag:\n",
    "            for i in range(rate_augment):\n",
    "                l.append(score)\n",
    "    y = np.array(l).flatten()\n",
    "    \n",
    "    # l_date_name = np.array(df['date'].unique())\n",
    "    # y = np.array(df[\"score\"]) #一番多いものをscoreとして最後にyとして返す\n",
    "    # y_arr = np.ones(l_date_name.shape)\n",
    "    # y_arr = y_arr * y\n",
    "    return y\n",
    "\n",
    "def interpolate1(array, length=100):\n",
    "    x_old = np.linspace(0, 1, array.shape[0])\n",
    "    y_old = array\n",
    "    \n",
    "    f = interpolate.interp1d(x_old, y_old)\n",
    "\n",
    "    x = np.linspace(0, 1, length)\n",
    "    y_new = f(x)\n",
    "    return y_new\n",
    "\n",
    "def zscore(x, axis = 1):\n",
    "    xmean = x.mean(axis=axis, keepdims=True)\n",
    "    xstd  = np.std(x, axis=axis, keepdims=True)\n",
    "    zscore = (x-xmean)/xstd\n",
    "    return zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xの系列を入力とすると，rateのものを10倍作成して返す\n",
    "def augmentation(arr_data, rate=0.8, num_arr=10, origin_length=100):\n",
    "    #まず100個に補間 shape:(100, num_feature)\n",
    "    interpolate = interpolate_one_sample(arr_data)\n",
    "    interpolate = interpolate.reshape(interpolate.shape[1], -1)\n",
    "    \n",
    "    num_data = origin_length * rate #実際に用いるデータ点列\n",
    "    \n",
    "    for i in range(num_arr):\n",
    "        first_idx = round( i/num_arr * origin_length * (1-rate))\n",
    "        end_idx = round( i/num_arr * origin_length * (1-rate) + origin_length * rate )\n",
    "        # print(f\"first index is {first_idx}, end index is {end_idx}\")\n",
    "        one_trimmed = interpolate[first_idx:end_idx, :]\n",
    "        one_arr = interpolate_one_sample(one_trimmed)\n",
    "        if i == 0:\n",
    "                x = one_arr #(1, 100, num_feature)\n",
    "        else:\n",
    "            x = np.concatenate([x, one_arr],0)\n",
    "    return x # (num_arr, 100, num_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_one_sample(arr_data):\n",
    "    num_feature = arr_data.shape[1]\n",
    "    for i in range(num_feature):\n",
    "            tmp_interpolate = interpolate1(arr_data[:, i])\n",
    "            #3次元に拡張\n",
    "            origin = tmp_interpolate.reshape(1, tmp_interpolate.shape[0], 1)\n",
    "            \n",
    "            if i == 0:\n",
    "                x = origin\n",
    "            else:\n",
    "                # print(f\"x shape is {x.shape}, \")\n",
    "                x = np.concatenate([x, origin],2)\n",
    "    return x #(1, 100, num_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# この関数を実行しさえすればデータセットが作成される\n",
    "def create_Xy(dataset, augment_flag=False, rate_augment=5, rate=0.9):\n",
    "    #正解ラベルの作成\n",
    "    y = extract_y(dataset, augment_flag, rate_augment=rate_augment)\n",
    "    X = None\n",
    "    \n",
    "    # Xの作成\n",
    "    l_date_name = np.array(dataset['date'].unique())\n",
    "    for k, date in enumerate(l_date_name):\n",
    "        #一連のデータ，1サンプル\n",
    "        series = dataset[dataset['date'] == l_date_name[k]]\n",
    "        series.drop(columns='point', inplace=True)\n",
    "        series.drop(columns='date', inplace=True)\n",
    "        #差分ベクトルを取得\n",
    "        diff = series.diff()\n",
    "        diff.fillna(0, inplace=True)\n",
    "\n",
    "        # 間接点同士のベクトルのカラムをデータフレームに追加\n",
    "        # これらがデータセットになる\n",
    "        diff = add_vector(diff, 'RElbow', \"RWrist\", 'v')\n",
    "        diff = add_vector(diff, 'RShoulder', \"RElbow\", 'v')\n",
    "        diff = add_vector(diff, 'Pelvis', \"RShoulder\", 'v')\n",
    "        diff = add_vector(diff, 'Pelvis', \"Neck\", 'v')\n",
    "        diff = add_vector(diff, 'Neck', 'RShoulder', 'v')\n",
    "        diff = add_vector(diff, 'Neck', 'LShoulder', 'v')\n",
    "        \n",
    "        # 1サンプルに関するデータ shape = (num_timestep, num_feature)\n",
    "        arr_data = diff.loc[:, \"v_x_RElbow2RWrist\":].values\n",
    "        \n",
    "        origin = interpolate_one_sample(arr_data)\n",
    "        \n",
    "        # augmentで増やす\n",
    "        # この関数の出力 (augmented_num, 100, num_feature)\n",
    "        if augment_flag:\n",
    "            augmented_arr = augmentation(arr_data, rate=rate, num_arr=rate_augment)\n",
    "            augmented_origin_arr = np.concatenate([origin, augmented_arr], 0)\n",
    "        else:\n",
    "            augmented_origin_arr = origin\n",
    "                \n",
    "        #最初だけ条件分岐\n",
    "        if k == 0:\n",
    "            out = augmented_origin_arr\n",
    "        else:\n",
    "            out = np.concatenate([out, augmented_origin_arr], axis=0)     \n",
    "    # 最後にzscore変換\n",
    "        # print(X.shape)\n",
    "    X = zscore(out)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxW659H6YTpz"
   },
   "source": [
    "## Build the model\n",
    "\n",
    "Our model processes a tensor of shape `(batch size, sequence length, features)`,\n",
    "where `sequence length` is the number of time steps and `features` is each input\n",
    "timeseries.\n",
    "\n",
    "You can replace your classification RNN layers with this one: the\n",
    "inputs are fully compatible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urNK4ym-YTp0"
   },
   "source": [
    "We include residual connections, layer normalization, and dropout.\n",
    "The resulting layer can be stacked multiple times.\n",
    "\n",
    "The projection layers are implemented through `keras.layers.Conv1D`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    '''入力された単語の位置を示すベクトル情報を付加する'''\n",
    "\n",
    "    def __init__(self, d_model=12, max_seq_len=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model  # 単語ベクトルの次元数\n",
    "\n",
    "        # 単語の順番（pos）と埋め込みベクトルの次元の位置（i）によって一意に定まる値の表をpeとして作成\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "\n",
    "        # GPUが使える場合はGPUへ送る、ここでは省略。実際に学習時には使用する\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        pe = pe.to(device)\n",
    "\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                \n",
    "                # 誤植修正_200510 #79\n",
    "                # pe[pos, i + 1] = math.cos(pos /\n",
    "                #                          (10000 ** ((2 * (i + 1))/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos /\n",
    "                                          (10000 ** ((2 * i)/d_model)))\n",
    "\n",
    "        # 表peの先頭に、ミニバッチ次元となる次元を足す\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "        # 勾配を計算しないようにする\n",
    "        self.pe.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 入力xとPositonal Encodingを足し算する\n",
    "        # xがpeよりも小さいので、大きくする\n",
    "        ret = math.sqrt(self.d_model)*x + self.pe\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # SAGANでは1dConvを使用したが、今回は全結合層で特徴量を変換する\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # 出力時に使用する全結合層\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Attentionの大きさ調整の変数\n",
    "        self.d_k = d_model\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        # 全結合層で特徴量を変換\n",
    "        k = self.k_linear(k)\n",
    "        q = self.q_linear(q)\n",
    "        v = self.v_linear(v)\n",
    "\n",
    "        # Attentionの値を計算する\n",
    "        # 各値を足し算すると大きくなりすぎるので、root(d_k)で割って調整\n",
    "        weights = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # ここでmaskを計算\n",
    "        # mask = mask.unsqueeze(1)\n",
    "        # print(mask)\n",
    "        # weights = weights.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # softmaxで規格化をする\n",
    "        normlized_weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "        # AttentionをValueとかけ算\n",
    "        output = torch.matmul(normlized_weights, v)\n",
    "\n",
    "        # 全結合層で特徴量を変換\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, normlized_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=1024, dropout=0.1):\n",
    "        '''Attention層から出力を単純に全結合層2つで特徴量を変換するだけのユニットです'''\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.dropout(F.relu(x))\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # LayerNormalization層\n",
    "        # https://pytorch.org/docs/stable/nn.html?highlight=layernorm\n",
    "        self.norm_1 = nn.LayerNorm(d_model)\n",
    "        self.norm_2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Attention層\n",
    "        self.attn = Attention(d_model)\n",
    "\n",
    "        # Attentionのあとの全結合層2つ\n",
    "        self.ff = FeedForward(d_model)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # 正規化とAttention\n",
    "        # print(f\"input shape is {x.shape}\")\n",
    "        x_normlized = self.norm_1(x)\n",
    "        output, attn_weights = self.attn(x_normlized, x_normlized, x_normlized, mask)\n",
    "        \n",
    "        x2 = x + self.dropout_1(output)\n",
    "\n",
    "        # 正規化と全結合層\n",
    "        x_normlized2 = self.norm_2(x2)\n",
    "        output = x2 + self.dropout_2(self.ff(x_normlized2))\n",
    "        # print(f\"output shape is {output.shape}\")\n",
    "\n",
    "        return output, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoringHead(nn.Module):\n",
    "    '''Transformer_Blockの出力を使用して，最後にスコアリングを行う'''\n",
    "    \n",
    "    def __init__(self, d_model=300, output_dim=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # 全結合層\n",
    "        self.linear = nn.Linear(d_model, output_dim)  # output_dimはポジ・ネガの2つ\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # 重み初期化処理\n",
    "        nn.init.normal_(self.linear.weight, std=0.02)\n",
    "        nn.init.normal_(self.linear.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = x[:, 0, :]  # 各ミニバッチの各文の先頭の単語の特徴量（300次元）を取り出す\n",
    "        weights = self.linear.weight\n",
    "        out = self.linear(x0)\n",
    "        \n",
    "        # print(f\"Scoring input shape is {x0.shape}\")\n",
    "        # print(f\"Scoring weight shape is {weights.shape}\")\n",
    "        # print(f\"Scoring output shape is {out.shape}\")\n",
    "\n",
    "        return out, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ClassificationHead(nn.Module):\n",
    "#     '''Transformer_Blockの出力を使用し、最後にクラス分類させる'''\n",
    "\n",
    "#     def __init__(self, d_model=300, output_dim=4):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # 全結合層\n",
    "#         self.linear = nn.Linear(d_model, output_dim)  # output_dimはポジ・ネガの2つ\n",
    "#         self.output_dim = output_dim\n",
    "\n",
    "#         # 重み初期化処理\n",
    "#         nn.init.normal_(self.linear.weight, std=0.02)\n",
    "#         nn.init.normal_(self.linear.bias, 0)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x0 = x[:, 0, :]  # 各ミニバッチの各文の先頭の単語の特徴量（300次元）を取り出す\n",
    "#         x1 = self.linear(x0)\n",
    "#         # print(x1.shape)\n",
    "#         out = F.softmax(x1, dim=-1)\n",
    "\n",
    "#         return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ここで複数入力のAttentionにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 最終的なTransformerモデルのクラス\n",
    "\n",
    "# class TransformerClassification(nn.Module):\n",
    "#     '''Transformerでクラス分類させる'''\n",
    "\n",
    "#     def __init__(self, d_model=12, max_seq_len=101, output_dim=4):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # モデル構築\n",
    "#         self.net_Positional = PositionalEncoder(d_model=d_model, max_seq_len=max_seq_len)\n",
    "        \n",
    "#         self.net_Attention_1 = TransformerBlock(d_model=4)\n",
    "#         self.net_Attention_2 = TransformerBlock(d_model=4)\n",
    "#         self.net_Attention_3 = TransformerBlock(d_model=4)\n",
    "#         self.net_Attention_4 = TransformerBlock(d_model=4)\n",
    "#         self.net_Attention_5 = TransformerBlock(d_model=4)\n",
    "#         self.net_Attention_6 = TransformerBlock(d_model=4)\n",
    "        \n",
    "#         self.net_Attention_all = TransformerBlock(d_model=24)\n",
    "#         # self.net3_2 = TransformerBlock(d_model=d_model)\n",
    "        \n",
    "#         self.net_Classification = ClassificationHead(output_dim=output_dim, d_model=d_model)\n",
    "\n",
    "#     def forward(self, x, mask):\n",
    "#         x = self.net_Positional(x)  # Positon情報を足し算\n",
    "#         #ここでいくつのAttentionアーキテクチャを利用するか決定する\n",
    "#         # num_attention_block = x.shape[2]/4\n",
    "        \n",
    "#         x1, attn_weights_1 = self.net_Attention_1(x[:,:,0:4], mask[:,:,0:4])  # Self-Attentionで特徴量を変換\n",
    "#         x2, attn_weights_2 = self.net_Attention_2(x[:,:,4:8], mask[:,:,4:8])\n",
    "#         x3, attn_weights_3 = self.net_Attention_3(x[:,:,8:12], mask[:,:,8:12])\n",
    "#         x4, attn_weights_4 = self.net_Attention_4(x[:,:,12:16], mask[:,:,12:16])\n",
    "#         x5, attn_weights_5 = self.net_Attention_5(x[:,:,16:20], mask[:,:,16:20])\n",
    "#         x6, attn_weights_6 = self.net_Attention_6(x[:,:,20:24], mask[:,:,20:24])\n",
    "        \n",
    "#         #出力を全部つなぎ合わせる\n",
    "#         x_concat = torch.cat((x1,x2,x3,x4,x5,x6), 2)\n",
    "#         # x_attn, attn_weights_all = self.net_Attention_all(x_concat, mask)\n",
    "        \n",
    "#         x_out = self.net_Classification(x_concat)  # 最終出力の0単語目を使用して、分類0-1のスカラーを出力\n",
    "        \n",
    "#         l_attn_weights = [attn_weights_1,attn_weights_2,attn_weights_3,attn_weights_4,attn_weights_5,attn_weights_6\n",
    "#                           # , attn_weights_all\n",
    "#                          ] \n",
    "#         return x_out, l_attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最終的なTransformerモデルのクラス\n",
    "\n",
    "class TransformerScoring(nn.Module):\n",
    "    '''Transformerでクラス分類させる'''\n",
    "\n",
    "    def __init__(self, d_model=12, max_seq_len=101, output_dim=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # モデル構築\n",
    "        self.net_Positional = PositionalEncoder(d_model=d_model, max_seq_len=max_seq_len)\n",
    "        \n",
    "        self.net_Attention_1 = TransformerBlock(d_model=4)\n",
    "        self.net_Attention_2 = TransformerBlock(d_model=4)\n",
    "        self.net_Attention_3 = TransformerBlock(d_model=4)\n",
    "        self.net_Attention_4 = TransformerBlock(d_model=4)\n",
    "        self.net_Attention_5 = TransformerBlock(d_model=4)\n",
    "        self.net_Attention_6 = TransformerBlock(d_model=4)\n",
    "        \n",
    "        self.net_Attention_all = TransformerBlock(d_model=24)\n",
    "        # self.net3_2 = TransformerBlock(d_model=d_model)\n",
    "        \n",
    "        # self.net_Classification = ClassificationHead(output_dim=output_dim, d_model=d_model)\n",
    "        self.net_Scoring = ScoringHead(output_dim=output_dim, d_model=d_model)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.net_Positional(x)  # Positon情報を足し算\n",
    "        #ここでいくつのAttentionアーキテクチャを利用するか決定する\n",
    "        # num_attention_block = x.shape[2]/4\n",
    "        \n",
    "        x1, attn_weights_1 = self.net_Attention_1(x[:,:,0:4], mask[:,:,0:4])  # Self-Attentionで特徴量を変換\n",
    "        x2, attn_weights_2 = self.net_Attention_2(x[:,:,4:8], mask[:,:,4:8])\n",
    "        x3, attn_weights_3 = self.net_Attention_3(x[:,:,8:12], mask[:,:,8:12])\n",
    "        x4, attn_weights_4 = self.net_Attention_4(x[:,:,12:16], mask[:,:,12:16])\n",
    "        x5, attn_weights_5 = self.net_Attention_5(x[:,:,16:20], mask[:,:,16:20])\n",
    "        x6, attn_weights_6 = self.net_Attention_6(x[:,:,20:24], mask[:,:,20:24])\n",
    "        \n",
    "        #出力を全部つなぎ合わせる\n",
    "        x_concat = torch.cat((x1,x2,x3,x4,x5,x6), 2)\n",
    "        x_attn, attn_weights_all = self.net_Attention_all(x_concat, mask)\n",
    "        x_out, _ = self.net_Scoring(x_attn)        \n",
    "        # x_out, _ = self.net_Scoring(x_concat)  # 最終出力の0単語目を使用して、分類0-1のスカラーを出力\n",
    "        \n",
    "        l_attn_weights = [attn_weights_1,attn_weights_2,attn_weights_3,attn_weights_4,attn_weights_5,attn_weights_6\n",
    "                          , attn_weights_all\n",
    "                         ] \n",
    "        return x_out, l_attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# validation用に1人のデータを検証データに回す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = dataset\n",
    "# [~(dataset['subject_id'] == 4)]\n",
    "# val_df = dataset[dataset['subject_id'] == 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PyTorchのdatasetの作成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, y_train = create_Xy(train_df, augment_flag=True, rate_augment=5, rate=0.95)\n",
    "# x_test, y_test = create_Xy(val_df, augment_flag=True, rate_augment=5, rate=0.95)\n",
    "\n",
    "# y_train = y_train.astype('int64')\n",
    "# y_test = y_test.astype('int64')\n",
    "\n",
    "\n",
    "# 正解データをone hotベクトルに変換\n",
    "# n_classes = len(np.unique(y_train))\n",
    "# y_train_onehot = np.identity(n_classes)[y_train]\n",
    "# y_test_onehot = np.identity(n_classes)[y_test]\n",
    "\n",
    "\n",
    "# ### 先頭にclassification用のトークンを追加する\n",
    "# score_train = np.zeros((x_train.shape[0], 1 , x_train.shape[2]))\n",
    "# x_train = np.concatenate([score_train, x_train], axis=1)\n",
    "\n",
    "# score_test = np.zeros((x_test.shape[0], 1 , x_test.shape[2]))\n",
    "# x_test = np.concatenate([score_test, x_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 先頭にclassification用のトークンを追加する\n",
    "# score_train = np.zeros((x_train.shape[0], 1 , x_train.shape[2]))\n",
    "# x_train = np.concatenate([score_train, x_train], axis=1)\n",
    "\n",
    "# score_test = np.zeros((x_test.shape[0], 1 , x_test.shape[2]))\n",
    "# x_test = np.concatenate([score_test, x_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LakZIHM8YTp1"
   },
   "source": [
    "## Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 動作確認\n",
    "# # ミニバッチの用意\n",
    "# batch = next(iter(train_dataloader))\n",
    "\n",
    "# # モデル構築\n",
    "\n",
    "# # 変数の固定\n",
    "# torch.manual_seed(1234)\n",
    "# torch.cuda.manual_seed(1234)\n",
    "\n",
    "# net = TransformerScoring(\n",
    "#     d_model=24, max_seq_len=101, output_dim=1)\n",
    "\n",
    "# # 入出力\n",
    "# _device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# net.to(_device)\n",
    "# x = batch[0].to(_device)\n",
    "# input_pad = 1\n",
    "# input_mask = (x != input_pad)\n",
    "# out, l_weights = net(x, input_mask)\n",
    "\n",
    "# print(\"出力のテンソルサイズ：\", out.shape)\n",
    "# # print(\"出力テンソル\", out)\n",
    "# # print(\"出力テンソルのsigmoid：\", F.softmax(out, dim=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elTTH_9KYTp0"
   },
   "source": [
    "The main part of our model is now complete. We can stack multiple of those\n",
    "`transformer_encoder` blocks and we can also proceed to add the final\n",
    "Multi-Layer Perceptron classification head. Apart from a stack of `Dense`\n",
    "layers, we need to reduce the output tensor of the `TransformerEncoder` part of\n",
    "our model down to a vector of features for each data point in the current\n",
    "batch. A common way to achieve this is to use a pooling layer. For\n",
    "this example, a `GlobalAveragePooling1D` layer is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ネットワークの初期化を定義\n",
    "\n",
    "# def weights_init(m):\n",
    "#     classname = m.__class__.__name__\n",
    "#     if classname.find('Linear') != -1:\n",
    "#         # Liner層の初期化\n",
    "#         nn.init.kaiming_normal_(m.weight)\n",
    "#         if m.bias is not None:\n",
    "#             nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "# # 訓練モードに設定\n",
    "# net.train()\n",
    "\n",
    "# # TransformerBlockモジュールを初期化実行\n",
    "\n",
    "# net.net_Attention_1.apply(weights_init)\n",
    "\n",
    "\n",
    "\n",
    "# print('ネットワーク設定完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数の設定\n",
    "# # criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.MSELoss()\n",
    "# # nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算\n",
    "\n",
    "# # 最適化手法の設定\n",
    "# learning_rate = 3e-4\n",
    "# optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs, patience, label):\n",
    "    # Early Stopping を定義\n",
    "    early_stopping = EarlyStopping(patience=patience, label=label)\n",
    "\n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"使用デバイス：\", device)\n",
    "    print('-----start-------')\n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # epochのループ\n",
    "    flag = False\n",
    "    for epoch in range(num_epochs):\n",
    "        # epochごとの訓練と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "        # for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "            else:\n",
    "                net.eval()   # モデルを検証モードに\n",
    "\n",
    "            epoch_loss = 0.0  # epochの損失和\n",
    "            epoch_corrects = 0  # epochの正解数\n",
    "\n",
    "            # データローダーからミニバッチを取り出すループ\n",
    "            for batch in (dataloaders_dict[phase]):\n",
    "                # batchはTextとLableの辞書オブジェクト\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                inputs = batch[0].to(device)  # 文章\n",
    "                labels = batch[1].to(device)  # ラベル\n",
    "\n",
    "                # optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                    # mask作成\n",
    "                    input_pad = 1  # 単語のIDにおいて、'<pad>': 1 なので\n",
    "                    input_mask = (inputs != input_pad)\n",
    "\n",
    "                    # Transformerに入力\n",
    "                    outputs, _ = net(inputs, input_mask)\n",
    "                    _, labels_true = torch.max(labels.data, 1)  # ワンホットラベルを平坦化\n",
    "                    labels_true_float = labels_true.float()\n",
    "                    # print(type(labels_true_float.dtype))\n",
    "                    # print(f\"outputs shape is {outputs.flatten().shape}, label shape is {labels_true.shape}\")\n",
    "                    outputs = outputs.flatten()\n",
    "                    loss = criterion(outputs, labels_true_float)  # 損失を計算\n",
    "                    # print(f\"output is {outputs}, labels float are {labels_true_float}\")\n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    # 結果の計算\n",
    "                    # print(np.array(preds))\n",
    "                    # print(np.array(np.argmax(labels.data)))\n",
    "                    # print(preds == labels.data)\n",
    "                    epoch_loss += loss.item() * inputs.size(0)  # lossの合計を更新\n",
    "                    # 正解数の合計を更新\n",
    "                    \n",
    "                    # one_hot=F.one_hot(preds,num_classes=4)\n",
    "                    # epoch_corrects += torch.sum(preds == labels_true)\n",
    "                    # print(labels_true)\n",
    "                    # print(torch.flatten(preds))\n",
    "                    preds = torch.round(outputs)  # ラベルを予測\n",
    "                    # print(f\"predicts are {preds}, labels {labels_true}\")\n",
    "                    # print(preds == labels_true)\n",
    "                    epoch_corrects += torch.sum(preds == labels_true)\n",
    "\n",
    "            # epochごとのlossと正解率\n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "            epoch_acc = epoch_corrects.double() / len(dataloaders_dict[phase].dataset)\n",
    "            \n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n",
    "                                                                           phase, epoch_loss, epoch_acc))\n",
    "                # print(f\"epoch corrects {epoch_corrects.double()}\")\n",
    "                # print(f\"dataset size = {len(dataloaders_dict[phase].dataset)}\")\n",
    "                \n",
    "            if phase == 'train':\n",
    "                writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "\n",
    "            else:\n",
    "                writer.add_scalar(\"Loss/test\", loss, epoch)\n",
    "                early_stopping(loss, net) # 最良モデルならモデルパラメータ保存\n",
    "                if early_stopping.early_stop: \n",
    "                # 一定epochだけval_lossが最低値を更新しなかった場合、ここに入り学習を終了\n",
    "                    print(f\"Early Stopping!! Epoch {epoch+1}/{num_epochs} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} :: {epoch_corrects.double()}/{len(dataloaders_dict[phase].dataset)}\")\n",
    "                    flag = True\n",
    "        if flag:\n",
    "            break\n",
    "\n",
    "    return net, epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #損失関数\n",
    "# criterion = nn.MSELoss()\n",
    "# num_epochs = 1000\n",
    "# patience = 100\n",
    "# batch_size = 32\n",
    "\n",
    "# n_classes = 4\n",
    "\n",
    "# # グリッドサーチを行う．\n",
    "# l_rate = [1e-5, 1e-4, 1e-3, 1e-2]\n",
    "# # l_rate = [1e-3]\n",
    "\n",
    "# seeds = [1000]\n",
    "# seed = 1000\n",
    "# # for i in range(15):\n",
    "# #     seeds.append(1000+i)\n",
    "\n",
    "\n",
    "# for seed in seeds:\n",
    "#     print(\"---------------------------------------------------------------------------------\")\n",
    "#     print(f\"seed is {seed}\")\n",
    "#     # 変数の固定\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "    \n",
    "#     for learning_rate in l_rate:\n",
    "\n",
    "#         # kf = KFold(n_splits=5, shuffle=True, random_state=2022)\n",
    "#         cv_loss = 0\n",
    "#         cv_acc = 0\n",
    "\n",
    "#         for subject_id in range(n_classes): \n",
    "#             print(\"---------------------------------------------------------------------------------\")\n",
    "#             print(f\"test subject id : {subject_id+1}\")\n",
    "#             train_df = dataset[~(dataset['subject_id'] == (subject_id+1))]\n",
    "#             val_df = dataset[dataset['subject_id'] == (subject_id+1)]\n",
    "\n",
    "#             x_train, y_train = create_Xy(train_df, augment_flag=True, rate_augment=5, rate=0.95)\n",
    "#             x_test, y_test = create_Xy(val_df, augment_flag=True, rate_augment=5, rate=0.95)\n",
    "\n",
    "#             ### 先頭にclassification用のトークンを追加する\n",
    "#             score_train = np.zeros((x_train.shape[0], 1 , x_train.shape[2]))\n",
    "#             x_train = np.concatenate([score_train, x_train], axis=1)\n",
    "#             score_test = np.zeros((x_test.shape[0], 1 , x_test.shape[2]))\n",
    "#             x_test = np.concatenate([score_test, x_test], axis=1)\n",
    "#             y_train_onehot = np.identity(n_classes)[y_train]\n",
    "#             y_test_onehot = np.identity(n_classes)[y_test]\n",
    "\n",
    "#             train_dataset = torch.utils.data.TensorDataset(torch.tensor(x_train, dtype=torch.float32)\n",
    "#                                                        , torch.tensor(y_train_onehot, dtype=torch.int8))\n",
    "#             val_dataset = torch.utils.data.TensorDataset(torch.tensor(x_test, dtype=torch.float32),\n",
    "#                                                          torch.tensor(y_test_onehot, dtype=torch.int8))\n",
    "#             train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "#             val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "#             dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
    "\n",
    "#             writer = SummaryWriter()\n",
    "#             print(f\"learning_rate is {learning_rate}\")\n",
    "\n",
    "#             # モデル構築\n",
    "#             net = TransformerScoring(d_model=24, max_seq_len=101, output_dim=1)\n",
    "#             # 訓練モードに設定\n",
    "#             net.train()\n",
    "#             optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "#             start = time.time() #時間計測スタート\n",
    "#             net_trained, loss, acc= train_model(net, \n",
    "#                                       dataloaders_dict,\n",
    "#                                       criterion,\n",
    "#                                       optimizer,\n",
    "#                                       num_epochs,\n",
    "#                                       patience\n",
    "#                                      )\n",
    "#             print(time.time() - start)\n",
    "#             writer.flush()\n",
    "#             cv_loss += loss / n_classes\n",
    "#             cv_acc += acc /n_classes\n",
    "#         print(f\"cv_acc is {cv_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# とりあえずvalidationがうまく動くかを確認\n",
    "\n",
    "uid = dataset[\"uid\"].unique()\n",
    "x_all, y_all = create_Xy(dataset, augment_flag=False)\n",
    "n_classes = len(np.unique(y_all))\n",
    "\n",
    "#損失関数\n",
    "criterion = nn.MSELoss()\n",
    "num_epochs = 1000\n",
    "patience = 100\n",
    "batch_size = 8\n",
    "\n",
    "# グリッドサーチを行う．\n",
    "l_rate = [1e-4]\n",
    "# l_rate = [1e-3]\n",
    "\n",
    "seeds = [1002, 1004, 1005, 1006, 1007, 1008, 1009, 1010]\n",
    "seed = 1000\n",
    "# for i in range(15):\n",
    "#     seeds.append(1000+i)\n",
    "\n",
    "# 変数の固定\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True,random_state=2022)\n",
    "\n",
    "for seed in seeds:\n",
    "    \n",
    "    for learning_rate in l_rate:\n",
    "        print(\"---------------------------------------------------------------------------------\")\n",
    "        cv = 0\n",
    "        for _fold, (train_index, test_index) in enumerate(skf.split(x_all, y_all)):  \n",
    "            \n",
    "            train_df = dataset[dataset[\"uid\"].isin(uid[train_index])]\n",
    "            val_df = dataset[dataset[\"uid\"].isin(uid[test_index])]\n",
    "            # print(train_df[\"uid\"].unique())\n",
    "            # print(val_df[\"uid\"].unique())\n",
    "\n",
    "            x_train, y_train = create_Xy(train_df, augment_flag=True, rate_augment=5, rate=0.95)\n",
    "            x_test, y_test = create_Xy(val_df, augment_flag=True, rate_augment=5, rate=0.95)\n",
    "\n",
    "            ### 先頭にclassification用のトークンを追加する\n",
    "            score_train = np.zeros((x_train.shape[0], 1 , x_train.shape[2]))\n",
    "            x_train = np.concatenate([score_train, x_train], axis=1)\n",
    "            score_test = np.zeros((x_test.shape[0], 1 , x_test.shape[2]))\n",
    "            x_test = np.concatenate([score_test, x_test], axis=1)\n",
    "            y_train_onehot = np.identity(n_classes)[y_train]\n",
    "            y_test_onehot = np.identity(n_classes)[y_test]\n",
    "\n",
    "            train_dataset = torch.utils.data.TensorDataset(torch.tensor(x_train, dtype=torch.float32)\n",
    "                                                       , torch.tensor(y_train_onehot, dtype=torch.int8))\n",
    "            val_dataset = torch.utils.data.TensorDataset(torch.tensor(x_test, dtype=torch.float32),\n",
    "                                                         torch.tensor(y_test_onehot, dtype=torch.int8))\n",
    "            train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "            val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "            dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
    "            \n",
    "            \n",
    "\n",
    "            writer = SummaryWriter()\n",
    "            print(f\"seed is {seed}\")\n",
    "            print(f\"learning_rate is {learning_rate}\")\n",
    "\n",
    "            # モデル構築\n",
    "            net = TransformerScoring(d_model=24, max_seq_len=101, output_dim=1)\n",
    "            # 訓練モードに設定\n",
    "            net.train()\n",
    "            optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "            start = time.time() #時間計測スタート\n",
    "            net_trained, loss , acc = train_model(net, \n",
    "                                      dataloaders_dict,\n",
    "                                      criterion,\n",
    "                                      optimizer,\n",
    "                                      num_epochs,\n",
    "                                      patience,\n",
    "                                      str(_fold)\n",
    "                                     )\n",
    "            print(time.time() - start)\n",
    "            writer.flush()\n",
    "            cv += acc / skf.n_splits\n",
    "        # torch.save(model.state_dict(), f\"models/crossvalidation_{_fold}.pth\")\n",
    "        print(f\"k fold accuracy:{cv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suzuk\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed is 1002\n",
      "learning_rate is 0.0001\n",
      "使用デバイス： cuda:0\n",
      "-----start-------\n",
      "Epoch 1/1000 | train |  Loss: 9.6657 Acc: 0.0000\n",
      "Epoch 1/1000 |  val  |  Loss: 8.6948 Acc: 0.0000\n",
      "Epoch 101/1000 | train |  Loss: 0.1195 Acc: 0.8468\n",
      "Epoch 101/1000 |  val  |  Loss: 0.8322 Acc: 0.4333\n",
      "Early Stopping!! Epoch 104/1000 Loss: 0.8327 Acc: 0.4000 :: 24.0/60\n",
      "36.35074043273926\n",
      "seed is 1002\n",
      "learning_rate is 0.0001\n",
      "使用デバイス： cuda:0\n",
      "-----start-------\n",
      "Epoch 1/1000 | train |  Loss: 9.3877 Acc: 0.0000\n",
      "Epoch 1/1000 |  val  |  Loss: 8.6813 Acc: 0.0000\n",
      "Epoch 101/1000 | train |  Loss: 0.1035 Acc: 0.8964\n",
      "Epoch 101/1000 |  val  |  Loss: 1.7600 Acc: 0.1167\n",
      "Early Stopping!! Epoch 105/1000 Loss: 1.6688 Acc: 0.1000 :: 6.0/60\n",
      "35.703280448913574\n",
      "seed is 1002\n",
      "learning_rate is 0.0001\n",
      "使用デバイス： cuda:0\n",
      "-----start-------\n",
      "Epoch 1/1000 | train |  Loss: 5.3753 Acc: 0.0965\n",
      "Epoch 1/1000 |  val  |  Loss: 4.7818 Acc: 0.2222\n",
      "Epoch 101/1000 | train |  Loss: 0.0915 Acc: 0.8904\n",
      "Epoch 101/1000 |  val  |  Loss: 1.2589 Acc: 0.5000\n",
      "Early Stopping!! Epoch 102/1000 Loss: 1.1417 Acc: 0.5185 :: 28.0/54\n",
      "36.76600742340088\n",
      "seed is 1002\n",
      "learning_rate is 0.0001\n",
      "使用デバイス： cuda:0\n",
      "-----start-------\n",
      "Epoch 1/1000 | train |  Loss: 1.5970 Acc: 0.2368\n",
      "Epoch 1/1000 |  val  |  Loss: 1.2867 Acc: 0.2222\n",
      "Epoch 101/1000 | train |  Loss: 0.0444 Acc: 0.9781\n",
      "Epoch 101/1000 |  val  |  Loss: 1.6865 Acc: 0.4074\n",
      "Epoch 201/1000 | train |  Loss: 0.0327 Acc: 0.9956\n",
      "Epoch 201/1000 |  val  |  Loss: 1.5790 Acc: 0.3889\n",
      "Epoch 301/1000 | train |  Loss: 0.0226 Acc: 0.9956\n",
      "Epoch 301/1000 |  val  |  Loss: 1.4624 Acc: 0.4444\n",
      "Early Stopping!! Epoch 349/1000 Loss: 1.4189 Acc: 0.3889 :: 21.0/54\n",
      "126.93656301498413\n",
      "seed is 1002\n",
      "learning_rate is 0.0001\n",
      "使用デバイス： cuda:0\n",
      "-----start-------\n",
      "Epoch 1/1000 | train |  Loss: 4.6091 Acc: 0.2588\n",
      "Epoch 1/1000 |  val  |  Loss: 3.5522 Acc: 0.2222\n",
      "Epoch 101/1000 | train |  Loss: 0.0607 Acc: 0.9561\n",
      "Epoch 101/1000 |  val  |  Loss: 1.5972 Acc: 0.4444\n",
      "Early Stopping!! Epoch 134/1000 Loss: 1.5695 Acc: 0.5000 :: 27.0/54\n",
      "49.0534291267395\n",
      "k fold accuracy:0.38148148148148153\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suzuk\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed is 1002\n",
      "learning_rate is 0.0005\n",
      "使用デバイス： cuda:0\n",
      "-----start-------\n",
      "Epoch 1/1000 | train |  Loss: 1.2304 Acc: 0.2838\n",
      "Epoch 1/1000 |  val  |  Loss: 1.4426 Acc: 0.2000\n",
      "Epoch 101/1000 | train |  Loss: 0.0286 Acc: 0.9955\n",
      "Epoch 101/1000 |  val  |  Loss: 1.3540 Acc: 0.2000\n",
      "Early Stopping!! Epoch 143/1000 Loss: 1.4648 Acc: 0.1833 :: 11.0/60\n",
      "49.469627141952515\n",
      "seed is 1002\n",
      "learning_rate is 0.0005\n",
      "使用デバイス： cuda:0\n",
      "-----start-------\n",
      "Epoch 1/1000 | train |  Loss: 11.5775 Acc: 0.0000\n",
      "Epoch 1/1000 |  val  |  Loss: 5.8176 Acc: 0.0000\n",
      "Epoch 101/1000 | train |  Loss: 0.0406 Acc: 0.9640\n",
      "Epoch 101/1000 |  val  |  Loss: 2.0865 Acc: 0.5333\n",
      "Early Stopping!! Epoch 103/1000 Loss: 1.9940 Acc: 0.4833 :: 29.0/60\n",
      "35.455079555511475\n",
      "seed is 1002\n",
      "learning_rate is 0.0005\n",
      "使用デバイス： cuda:0\n",
      "-----start-------\n",
      "Epoch 1/1000 | train |  Loss: 1.6064 Acc: 0.2325\n",
      "Epoch 1/1000 |  val  |  Loss: 1.2173 Acc: 0.3333\n",
      "Epoch 101/1000 | train |  Loss: 0.0253 Acc: 0.9956\n",
      "Epoch 101/1000 |  val  |  Loss: 1.1620 Acc: 0.3889\n",
      "Early Stopping!! Epoch 130/1000 Loss: 1.2080 Acc: 0.4074 :: 22.0/54\n",
      "47.368093490600586\n",
      "seed is 1002\n",
      "learning_rate is 0.0005\n",
      "使用デバイス： cuda:0\n",
      "-----start-------\n",
      "Epoch 1/1000 | train |  Loss: 6.1107 Acc: 0.1360\n",
      "Epoch 1/1000 |  val  |  Loss: 1.3724 Acc: 0.3333\n",
      "Epoch 101/1000 | train |  Loss: 0.0428 Acc: 0.9825\n",
      "Epoch 101/1000 |  val  |  Loss: 1.1789 Acc: 0.5185\n",
      "Early Stopping!! Epoch 101/1000 Loss: 1.1789 Acc: 0.5185 :: 28.0/54\n",
      "36.717015743255615\n",
      "seed is 1002\n",
      "learning_rate is 0.0005\n",
      "使用デバイス： cuda:0\n",
      "-----start-------\n",
      "Epoch 1/1000 | train |  Loss: 1.6144 Acc: 0.2237\n",
      "Epoch 1/1000 |  val  |  Loss: 1.1879 Acc: 0.2222\n",
      "Epoch 101/1000 | train |  Loss: 0.0265 Acc: 1.0000\n",
      "Epoch 101/1000 |  val  |  Loss: 0.6383 Acc: 0.5926\n",
      "Early Stopping!! Epoch 107/1000 Loss: 0.5935 Acc: 0.4444 :: 24.0/54\n",
      "39.321778774261475\n",
      "k fold accuracy:0.40740740740740744\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suzuk\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed is 1002\n",
      "learning_rate is 0.001\n",
      "使用デバイス： cuda:0\n",
      "-----start-------\n",
      "Epoch 1/1000 | train |  Loss: 2.8858 Acc: 0.1892\n",
      "Epoch 1/1000 |  val  |  Loss: 2.1520 Acc: 0.2000\n",
      "Epoch 101/1000 | train |  Loss: 0.0219 Acc: 0.9955\n",
      "Epoch 101/1000 |  val  |  Loss: 0.7818 Acc: 0.4000\n",
      "Epoch 201/1000 | train |  Loss: 0.0168 Acc: 0.9955\n",
      "Epoch 201/1000 |  val  |  Loss: 0.7830 Acc: 0.3500\n",
      "Early Stopping!! Epoch 239/1000 Loss: 0.7623 Acc: 0.3667 :: 22.0/60\n",
      "82.30905818939209\n",
      "seed is 1002\n",
      "learning_rate is 0.001\n",
      "使用デバイス： cuda:0\n",
      "-----start-------\n",
      "Epoch 1/1000 | train |  Loss: 1.9837 Acc: 0.2072\n",
      "Epoch 1/1000 |  val  |  Loss: 1.5378 Acc: 0.2000\n",
      "Epoch 101/1000 | train |  Loss: 0.0130 Acc: 1.0000\n",
      "Epoch 101/1000 |  val  |  Loss: 1.1651 Acc: 0.2667\n",
      "Early Stopping!! Epoch 107/1000 Loss: 1.1480 Acc: 0.2833 :: 17.0/60\n",
      "36.91885018348694\n",
      "seed is 1002\n",
      "learning_rate is 0.001\n",
      "使用デバイス： cuda:0\n",
      "-----start-------\n",
      "Epoch 1/1000 | train |  Loss: 1.3339 Acc: 0.2632\n",
      "Epoch 1/1000 |  val  |  Loss: 1.2635 Acc: 0.2222\n",
      "Epoch 101/1000 | train |  Loss: 0.0166 Acc: 1.0000\n",
      "Epoch 101/1000 |  val  |  Loss: 1.0671 Acc: 0.0185\n",
      "Early Stopping!! Epoch 119/1000 Loss: 1.0258 Acc: 0.0185 :: 1.0/54\n",
      "43.77145838737488\n",
      "seed is 1002\n",
      "learning_rate is 0.001\n",
      "使用デバイス： cuda:0\n",
      "-----start-------\n",
      "Epoch 1/1000 | train |  Loss: 2.0996 Acc: 0.1886\n",
      "Epoch 1/1000 |  val  |  Loss: 1.2193 Acc: 0.2222\n",
      "Epoch 101/1000 | train |  Loss: 0.0223 Acc: 1.0000\n",
      "Epoch 101/1000 |  val  |  Loss: 1.4253 Acc: 0.1667\n",
      "Epoch 201/1000 | train |  Loss: 0.0139 Acc: 1.0000\n",
      "Epoch 201/1000 |  val  |  Loss: 1.4577 Acc: 0.2037\n",
      "Early Stopping!! Epoch 292/1000 Loss: 1.2375 Acc: 0.2963 :: 16.0/54\n",
      "106.25888109207153\n",
      "seed is 1002\n",
      "learning_rate is 0.001\n",
      "使用デバイス： cuda:0\n",
      "-----start-------\n",
      "Epoch 1/1000 | train |  Loss: 2.0240 Acc: 0.2500\n",
      "Epoch 1/1000 |  val  |  Loss: 1.3422 Acc: 0.2222\n",
      "Epoch 101/1000 | train |  Loss: 0.0358 Acc: 0.9956\n",
      "Epoch 101/1000 |  val  |  Loss: 1.8251 Acc: 0.3704\n",
      "Epoch 201/1000 | train |  Loss: 0.0172 Acc: 1.0000\n",
      "Epoch 201/1000 |  val  |  Loss: 1.9138 Acc: 0.3704\n",
      "Early Stopping!! Epoch 298/1000 Loss: 1.8086 Acc: 0.3519 :: 19.0/54\n",
      "110.69487261772156\n",
      "k fold accuracy:0.26333333333333336\n"
     ]
    }
   ],
   "source": [
    "# とりあえずvalidationがうまく動くかを確認\n",
    "\n",
    "uid = dataset[\"uid\"].unique()\n",
    "x_all, y_all = create_Xy(dataset, augment_flag=False)\n",
    "n_classes = len(np.unique(y_all))\n",
    "\n",
    "#損失関数\n",
    "criterion = nn.MSELoss()\n",
    "num_epochs = 1000\n",
    "patience = 100\n",
    "batch_size = 16\n",
    "\n",
    "# グリッドサーチを行う．\n",
    "l_rate = [1e-4, 5e-4, 1e-3]\n",
    "# l_rate = [1e-3]\n",
    "\n",
    "# seeds = [1002, 1004, 1005, 1006, 1007, 1008, 1009, 1010]\n",
    "seeds = [1002]\n",
    "seed = 1000\n",
    "# for i in range(15):\n",
    "#     seeds.append(1000+i)\n",
    "\n",
    "# 変数の固定\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True,random_state=8888)\n",
    "\n",
    "for seed in seeds:\n",
    "    \n",
    "    for learning_rate in l_rate:\n",
    "        print(\"---------------------------------------------------------------------------------\")\n",
    "        cv = 0\n",
    "        for _fold, (train_index, test_index) in enumerate(skf.split(x_all, y_all)):  \n",
    "            \n",
    "            train_df = dataset[dataset[\"uid\"].isin(uid[train_index])]\n",
    "            val_df = dataset[dataset[\"uid\"].isin(uid[test_index])]\n",
    "            # print(train_df[\"uid\"].unique())\n",
    "            # print(val_df[\"uid\"].unique())\n",
    "\n",
    "            x_train, y_train = create_Xy(train_df, augment_flag=True, rate_augment=5, rate=0.95)\n",
    "            x_test, y_test = create_Xy(val_df, augment_flag=True, rate_augment=5, rate=0.95)\n",
    "\n",
    "            ### 先頭にclassification用のトークンを追加する\n",
    "            score_train = np.zeros((x_train.shape[0], 1 , x_train.shape[2]))\n",
    "            x_train = np.concatenate([score_train, x_train], axis=1)\n",
    "            score_test = np.zeros((x_test.shape[0], 1 , x_test.shape[2]))\n",
    "            x_test = np.concatenate([score_test, x_test], axis=1)\n",
    "            y_train_onehot = np.identity(n_classes)[y_train]\n",
    "            y_test_onehot = np.identity(n_classes)[y_test]\n",
    "\n",
    "            train_dataset = torch.utils.data.TensorDataset(torch.tensor(x_train, dtype=torch.float32)\n",
    "                                                       , torch.tensor(y_train_onehot, dtype=torch.int8))\n",
    "            val_dataset = torch.utils.data.TensorDataset(torch.tensor(x_test, dtype=torch.float32),\n",
    "                                                         torch.tensor(y_test_onehot, dtype=torch.int8))\n",
    "            train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "            val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "            dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
    "            \n",
    "            \n",
    "\n",
    "            writer = SummaryWriter()\n",
    "            print(f\"seed is {seed}\")\n",
    "            print(f\"learning_rate is {learning_rate}\")\n",
    "\n",
    "            # モデル構築\n",
    "            net = TransformerScoring(d_model=24, max_seq_len=101, output_dim=1)\n",
    "            # 訓練モードに設定\n",
    "            net.train()\n",
    "            optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "            start = time.time() #時間計測スタート\n",
    "            net_trained, loss , acc = train_model(net, \n",
    "                                      dataloaders_dict,\n",
    "                                      criterion,\n",
    "                                      optimizer,\n",
    "                                      num_epochs,\n",
    "                                      patience,\n",
    "                                      str(_fold)\n",
    "                                     )\n",
    "            print(time.time() - start)\n",
    "            writer.flush()\n",
    "            cv += acc / skf.n_splits\n",
    "        # torch.save(model.state_dict(), f\"models/crossvalidation_{_fold}.pth\")\n",
    "        print(f\"k fold accuracy:{cv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_df = dataset\n",
    "\n",
    "x_all, y_all = create_Xy(all_df, augment_flag=False, rate_augment=5, rate=0.95)\n",
    "\n",
    "# 正解データをone hotベクトルに変換\n",
    "n_classes = len(np.unique(y_all))\n",
    "y_all_onehot = np.identity(n_classes)[y_all]\n",
    "\n",
    "### 先頭にclassification用のトークンを追加する\n",
    "score_train = np.zeros((x_all.shape[0], 1 , x_all.shape[2]))\n",
    "x_all = np.concatenate([score_train, x_all], axis=1)\n",
    "\n",
    "dataset_all = torch.utils.data.TensorDataset(torch.tensor(x_all, dtype=torch.float32)\n",
    "                                               , torch.tensor(y_all_onehot, dtype=torch.int8))\n",
    "\n",
    "\n",
    "#損失関数\n",
    "criterion = nn.MSELoss()\n",
    "num_epochs = 1000\n",
    "patience = 100\n",
    "batch_size = 8\n",
    "\n",
    "# グリッドサーチを行う．\n",
    "l_rate = [5e-4]\n",
    "# l_rate = [1e-3]\n",
    "\n",
    "seeds = [1000]\n",
    "seed = 1000\n",
    "# for i in range(15):\n",
    "#     seeds.append(1000+i)\n",
    "\n",
    "# 変数の固定\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "train_index, valid_index = train_test_split(range(len(dataset_all)), test_size=0.2)\n",
    "\n",
    "for seed in seeds:\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=2022)\n",
    "    for learning_rate in l_rate:\n",
    "        print(\"---------------------------------------------------------------------------------\")\n",
    "        cv = 0\n",
    "        for _fold, (train_index, test_index) in enumerate(kf.split(dataset_all)):  \n",
    "            \n",
    "            train_dataset = Subset(dataset_all, train_index)\n",
    "            train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "            val_dataset = Subset(dataset_all, valid_index)\n",
    "            val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "            dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
    "\n",
    "            print(f\"seed is {seed}\")\n",
    "\n",
    "            writer = SummaryWriter()\n",
    "            print(f\"learning_rate is {learning_rate}\")\n",
    "\n",
    "            # モデル構築\n",
    "            net = TransformerScoring(d_model=24, max_seq_len=101, output_dim=1)\n",
    "            # 訓練モードに設定\n",
    "            net.train()\n",
    "            optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "            start = time.time() #時間計測スタート\n",
    "            net_trained, loss , acc = train_model(net, \n",
    "                                      dataloaders_dict,\n",
    "                                      criterion,\n",
    "                                      optimizer,\n",
    "                                      num_epochs,\n",
    "                                      patience\n",
    "                                     )\n",
    "            print(time.time() - start)\n",
    "            writer.flush()\n",
    "            cv += acc / kf.n_splits\n",
    "        print(f\"k fold accuracy:{cv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 学習・検証を実行する 15分ほどかかります\n",
    "# net = TransformerClassification(d_model=24, max_seq_len=101, output_dim=4)\n",
    "# # 訓練モードに設定\n",
    "# net.train()\n",
    "\n",
    "# # 損失関数の設定\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# # nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算\n",
    "\n",
    "# # 最適化手法の設定\n",
    "# learning_rate = 2e-4\n",
    "# optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# print('ネットワーク設定完了')\n",
    "\n",
    "# num_epochs = 300\n",
    "# net_trained = train_model(net, dataloaders_dict,\n",
    "#                           criterion, optimizer, num_epochs=num_epochs)\n",
    "# writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZN2SeFtLHbyT"
   },
   "source": [
    "# SaveModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テストデータ228個での正解率：1.0000\n"
     ]
    }
   ],
   "source": [
    "# device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 新しいモデル\n",
    "net_trained.eval()\n",
    "net_trained.to(device)\n",
    "\n",
    "epoch_corrects = 0  # epochの正解数\n",
    "\n",
    "# test_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=300)\n",
    "test_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=300)\n",
    "\n",
    "for batch in (test_dataloader):  # testデータのDataLoader\n",
    "    # batchはTextとLableの辞書オブジェクト\n",
    "    \n",
    "    # GPUが使えるならGPUにデータを送る\n",
    "    inputs = batch[0].to(device)  # 文章\n",
    "    labels = batch[1].to(device)  # ラベル\n",
    "\n",
    "    # 順伝搬（forward）計算\n",
    "    with torch.set_grad_enabled(False):\n",
    "\n",
    "        # mask作成\n",
    "        input_pad = 1  # 単語のIDにおいて、'<pad>': 1 なので\n",
    "        input_mask = (inputs != input_pad)\n",
    "\n",
    "        # Transformerに入力\n",
    "        outputs, l_attention_weights = net_trained(inputs, input_mask)\n",
    "        _, labels_true = torch.max(labels.data, 1)  # ワンホットラベルを平坦化\n",
    "        outputs = outputs.flatten()\n",
    "\n",
    "        # 正解数の合計を更新\n",
    "        preds = torch.round(outputs)  # ラベルを予測\n",
    "        # print(f\"predicts are {preds}, labels {labels_true}\")\n",
    "        epoch_corrects += torch.sum(preds == labels_true)\n",
    "\n",
    "# 正解率\n",
    "epoch_acc = epoch_corrects.double() / len(test_dataloader.dataset)\n",
    "\n",
    "print('テストデータ{}個での正解率：{:.4f}'.format(len(test_dataloader.dataset),epoch_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "preds_heat = np.array(preds.cpu())\n",
    "trues_heat = np.array(labels_true.cpu())\n",
    "# preds_heat[0] = \"3A\"\n",
    "# print(preds_heat)\n",
    "# print(trues_heat)\n",
    "\n",
    "heat = confusion_matrix(trues_heat, preds_heat)\n",
    "# print(heat)\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"   # 使用するフォント\n",
    "plt.rcParams[\"font.size\"] = 20        \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "xtics = [\"2C\", \"2B\", \"2A\", \"3\"]\n",
    "ytics = [\"2C\", \"2B\", \"2A\", \"3\"]\n",
    "# Set the palette using the name of a palette:\n",
    "sns.heatmap(heat, annot=True, xticklabels=xtics, yticklabels=ytics, cmap=\"Blues\", cbar=False)\n",
    "# sns.heatmap(heat, annot=True, cmap=\"Blues\", cbar=False)\n",
    "#*以下2行がポイント*  X,Y軸ラベルを追加\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "#グラフをはみ出さないようにして画面に出力\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"nn-heatmap_crossvalid.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプルごとに代償動作の度合いをセッティングしてみる\n",
    "preds = np.array(outputs.cpu())\n",
    "ans = np.array(labels_true.cpu())\n",
    "plt.scatter(preds, ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l_attention_weights[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 25\n",
    "\n",
    "print(f\"predicts are {preds[index]}, labels {labels_true[index]}\")\n",
    "\n",
    "RElbow2RWrist_weight = np.array(l_attention_weights[0].cpu()[index, 0, 1:]).reshape(1, -1)\n",
    "RShoulder2RElbow_weight = np.array(l_attention_weights[1].cpu()[index, 0, 1:]).reshape(1, -1)\n",
    "Pelvis2RShoulder_weight = np.array(l_attention_weights[2].cpu()[index, 0, 1:]).reshape(1, -1)\n",
    "Pelvis2RNeck_weight = np.array(l_attention_weights[3].cpu()[index, 0, 1:]).reshape(1, -1)\n",
    "Nect2RShoulder_weight = np.array(l_attention_weights[4].cpu()[index, 0, 1:]).reshape(1, -1)\n",
    "Neck2LShoulder_weight = np.array(l_attention_weights[5].cpu()[index, 0, 1:]).reshape(1, -1)\n",
    "\n",
    "weights = np.concatenate([RElbow2RWrist_weight, RShoulder2RElbow_weight, Pelvis2RShoulder_weight, Pelvis2RNeck_weight,\n",
    "                         Nect2RShoulder_weight, Neck2LShoulder_weight])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "# Set the palette using the name of a palette:\n",
    "# sns.heatmap(RElbow2RWrist_weight, cmap='OrRd')\n",
    "# sns.heatmap(RShoulder2RElbow_weight, cmap='OrRd')\n",
    "# sns.heatmap(Pelvis2RShoulder_weight, cmap='OrRd')\n",
    "# sns.heatmap(Pelvis2RNeck_weight, cmap='OrRd')\n",
    "# sns.heatmap(Nect2RShoulder_weight, cmap='OrRd')\n",
    "# sns.heatmap(Neck2LShoulder_weight, cmap='OrRd')\n",
    "\n",
    "sns.heatmap(weights, cmap='OrRd')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "extract_data = np.array(X[19, 1:, 2]).flatten()\n",
    "print(extract_data.shape)\n",
    "\n",
    "\n",
    "time = np.linspace(0, 100, 100)\n",
    "fig, ax = plt.subplots(2, 1, gridspec_kw={'height_ratios': [6, 1]}, figsize=(15,10))\n",
    "# Set the palette using the name of a palette:\n",
    "sns.lineplot(time, extract_data, ax=ax[0])\n",
    "sns.heatmap(extract_weight, cmap='OrRd', cbar=False, ax=ax[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "data = np.array(attn_weights1[:, 0, :])\n",
    "print(data.shape)\n",
    "data = data.reshape(attn_weights1.shape[0], -1)\n",
    "\n",
    "# print(outputs)\n",
    "print(data.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30, 20))\n",
    "# Set the palette using the name of a palette:\n",
    "sns.heatmap(data, cmap='OrRd')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data[0])):\n",
    "    print(data[0,i], end=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_trained.net_Attention_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelの読み込みだけ\n",
    "# 保存したモデルパラメータの読み込み\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net_trained = TransformerClassification().to(device)\n",
    "\n",
    "net_trained.load_state_dict(torch.load('highacc2.pth'))\n",
    "# print('読み込み後のモデル:\\n', model2.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(net_trained.state_dict(), 'highacc3.pth')\n",
    "\n",
    "# 新しいモデル\n",
    "model2 = TransformerClassification().to(device)\n",
    "print('新しいモデル:\\n', model2.state_dict())\n",
    "\n",
    "# 保存したモデルパラメータの読み込み\n",
    "model2.load_state_dict(torch.load('highacc3.pth'))\n",
    "print('読み込み後のモデル:\\n', model2.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attention weight の保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape\n",
    "print(data)\n",
    "print(data.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 同時に入れるデータも作成する\n",
    "# これは時系列スタンプ．0〜100がサンプル数だけ作られる\n",
    "stamp_row = [i%data.shape[1] for i in range(data.shape[0] * data.shape[1])]\n",
    "\n",
    "# いつ撮影されたのかのスタンプ\n",
    "l_date_name = np.array(df_annotate['date'].unique())\n",
    "date_row = []\n",
    "for item in l_date_name:\n",
    "    for i in range(data.shape[1]):\n",
    "        date_row.append(item)\n",
    "        \n",
    "# アテンションウエイト\n",
    "attn_weight = data.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attn = pd.DataFrame()\n",
    "df_attn['date'] = date_row\n",
    "df_attn[\"stamp\"] = stamp_row\n",
    "df_attn[\"attn_weight\"] = attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attn.to_excel('/Users/kento/kuhp/experiment/attention_weights.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_attn['date'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "timeseries_transformer_classification.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/keras-team/keras-io/blob/master/examples/timeseries/ipynb/timeseries_classification_transformer.ipynb",
     "timestamp": 1636355482830
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
