{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyRQwYrEYTpv"
   },
   "source": [
    "# 修士論文実験 2022/01/10\n",
    "## マルチモーダルに挑戦\n",
    "# 代償度合いを用いて判断を行う。\n",
    "## (validationが簡単に行えるようにデータセットを操作するモジュールを拡充)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 843,
     "status": "ok",
     "timestamp": 1636987406075,
     "user": {
      "displayName": "Kento Suzuki",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16345288529127918743"
     },
     "user_tz": -540
    },
    "id": "dO5GxI7RzXIc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import Subset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from pytorchtools import EarlyStopping\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# sys.path.append(os.path.abspath(\"..\"))\n",
    "# import scripts.compensate\n",
    "# import scripts.kinectImg2video\n",
    "# from scripts.conpemsate_suppresser import *\n",
    "# from scripts.ground_angle_analysis import ground_shoulder_angle_analyzer\n",
    "# import scripts.ground_angle_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup seeds\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日付の追加\n",
    "df_annotate = pd.read_excel(\"annotate_dataset.xlsx\")\n",
    "df_annotate['date'] = pd.to_datetime(df_annotate['date'], format='%Y-%m-%d-%H-%M-%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>uid</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>task_id</th>\n",
       "      <th>point</th>\n",
       "      <th>shoulder</th>\n",
       "      <th>body</th>\n",
       "      <th>flag_usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-06 18:56:20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-12-06 18:56:20</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-12-06 18:56:41</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-12-06 18:56:41</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-06 18:56:58</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date  uid  subject_id  task_id point  shoulder  body  \\\n",
       "0 2021-12-06 18:56:20    1           1        1     3         0     0   \n",
       "1 2021-12-06 18:56:20    2           1        1     4         0     0   \n",
       "2 2021-12-06 18:56:41    3           1        1     3         0     0   \n",
       "3 2021-12-06 18:56:41    4           1        1     4         0     0   \n",
       "4 2021-12-06 18:56:58    5           1        1     3         0     0   \n",
       "\n",
       "   flag_usage  \n",
       "0           1  \n",
       "1           2  \n",
       "2           1  \n",
       "3           2  \n",
       "4           1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annotate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>id</th>\n",
       "      <th>x_Pelvis</th>\n",
       "      <th>y_Pelvis</th>\n",
       "      <th>z_Pelvis</th>\n",
       "      <th>x_SpineNaval</th>\n",
       "      <th>y_SpineNaval</th>\n",
       "      <th>z_SpineNaval</th>\n",
       "      <th>x_SpineChest</th>\n",
       "      <th>...</th>\n",
       "      <th>x_REar</th>\n",
       "      <th>y_REar</th>\n",
       "      <th>z_REar</th>\n",
       "      <th>N</th>\n",
       "      <th>v_RWrist</th>\n",
       "      <th>z_v_RWrist</th>\n",
       "      <th>flag_active</th>\n",
       "      <th>flag_moving</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-158.973129</td>\n",
       "      <td>353.261841</td>\n",
       "      <td>698.233093</td>\n",
       "      <td>-149.886017</td>\n",
       "      <td>218.100922</td>\n",
       "      <td>780.778748</td>\n",
       "      <td>-142.035919</td>\n",
       "      <td>...</td>\n",
       "      <td>-178.933960</td>\n",
       "      <td>-236.272278</td>\n",
       "      <td>842.268127</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.917988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2021-12-04 18:20:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-156.911919</td>\n",
       "      <td>356.031863</td>\n",
       "      <td>697.378637</td>\n",
       "      <td>-147.957493</td>\n",
       "      <td>219.063530</td>\n",
       "      <td>781.238292</td>\n",
       "      <td>-142.348493</td>\n",
       "      <td>...</td>\n",
       "      <td>-176.788628</td>\n",
       "      <td>-241.853940</td>\n",
       "      <td>862.009244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.131710</td>\n",
       "      <td>0.480634</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.891892</td>\n",
       "      <td>2021-12-04 18:20:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-157.406120</td>\n",
       "      <td>356.970863</td>\n",
       "      <td>697.867036</td>\n",
       "      <td>-148.184343</td>\n",
       "      <td>219.214968</td>\n",
       "      <td>781.799317</td>\n",
       "      <td>-142.899719</td>\n",
       "      <td>...</td>\n",
       "      <td>-176.093690</td>\n",
       "      <td>-243.703880</td>\n",
       "      <td>867.831643</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.690953</td>\n",
       "      <td>0.198082</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79.783784</td>\n",
       "      <td>2021-12-04 18:20:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-159.493486</td>\n",
       "      <td>356.687710</td>\n",
       "      <td>699.214911</td>\n",
       "      <td>-149.753131</td>\n",
       "      <td>218.849632</td>\n",
       "      <td>782.412353</td>\n",
       "      <td>-143.553739</td>\n",
       "      <td>...</td>\n",
       "      <td>-176.457427</td>\n",
       "      <td>-243.071735</td>\n",
       "      <td>864.096621</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.280899</td>\n",
       "      <td>0.313779</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>119.675676</td>\n",
       "      <td>2021-12-04 18:20:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-162.211771</td>\n",
       "      <td>355.791270</td>\n",
       "      <td>700.938883</td>\n",
       "      <td>-151.850422</td>\n",
       "      <td>218.261918</td>\n",
       "      <td>783.027933</td>\n",
       "      <td>-144.174692</td>\n",
       "      <td>...</td>\n",
       "      <td>-177.488121</td>\n",
       "      <td>-241.207140</td>\n",
       "      <td>855.165480</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.247976</td>\n",
       "      <td>0.307322</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>159.567568</td>\n",
       "      <td>2021-12-04 18:20:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1   id    x_Pelvis    y_Pelvis    z_Pelvis  \\\n",
       "0           0             0  1.0 -158.973129  353.261841  698.233093   \n",
       "1           1             1  1.0 -156.911919  356.031863  697.378637   \n",
       "2           2             2  1.0 -157.406120  356.970863  697.867036   \n",
       "3           3             3  1.0 -159.493486  356.687710  699.214911   \n",
       "4           4             4  1.0 -162.211771  355.791270  700.938883   \n",
       "\n",
       "   x_SpineNaval  y_SpineNaval  z_SpineNaval  x_SpineChest  ...      x_REar  \\\n",
       "0   -149.886017    218.100922    780.778748   -142.035919  ... -178.933960   \n",
       "1   -147.957493    219.063530    781.238292   -142.348493  ... -176.788628   \n",
       "2   -148.184343    219.214968    781.799317   -142.899719  ... -176.093690   \n",
       "3   -149.753131    218.849632    782.412353   -143.553739  ... -176.457427   \n",
       "4   -151.850422    218.261918    783.027933   -144.174692  ... -177.488121   \n",
       "\n",
       "       y_REar      z_REar   N  v_RWrist  z_v_RWrist  flag_active  flag_moving  \\\n",
       "0 -236.272278  842.268127 NaN       NaN   -0.917988          0.0          1.0   \n",
       "1 -241.853940  862.009244 NaN  7.131710    0.480634          1.0          1.0   \n",
       "2 -243.703880  867.831643 NaN  5.690953    0.198082          1.0          1.0   \n",
       "3 -243.071735  864.096621 NaN  6.280899    0.313779          1.0          1.0   \n",
       "4 -241.207140  855.165480 NaN  6.247976    0.307322          1.0          1.0   \n",
       "\n",
       "    timestamp                date  \n",
       "0    0.000000 2021-12-04 18:20:18  \n",
       "1   39.891892 2021-12-04 18:20:18  \n",
       "2   79.783784 2021-12-04 18:20:18  \n",
       "3  119.675676 2021-12-04 18:20:18  \n",
       "4  159.567568 2021-12-04 18:20:18  \n",
       "\n",
       "[5 rows x 106 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataSetの呼び出し\n",
    "df_data = pd.read_csv(\"bigdata_trimmed.csv\")\n",
    "df_data['date'] = pd.to_datetime(df_data['date'], format='%Y-%m-%d-%H-%M-%S')\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1636987406076,
     "user": {
      "displayName": "Kento Suzuki",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16345288529127918743"
     },
     "user_tz": -540
    },
    "id": "GZlVgJy1zXIc"
   },
   "outputs": [],
   "source": [
    "# pd_merged = pd.merge(df_annotate, df_data, on='date', how='inner')\n",
    "# pd_3 = pd_merged[pd_merged['point'] == 3]\n",
    "# pd_2A = pd_merged[pd_merged['point'] == '2A']\n",
    "# pd_2B = pd_merged[pd_merged['point'] == '2B']\n",
    "# pd_2C = pd_merged[pd_merged['point'] == '2C']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### このデータセットはGraspのものかつ，usageが1のものが取り出されている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットにNN入力用のスコアを割り振る．\n",
    "# これが正解ラベルになる\n",
    "dataset = pd.merge(df_annotate, df_data, on='date', how='inner')\n",
    "\n",
    "# dataset = dataset[(dataset['task_id'] == 1) & (dataset['flag_usage'] == 1)]\n",
    "dataset = dataset[(dataset['task_id'] == 1) & (dataset['flag_usage'] == 2)]\n",
    "dataset['point'] = dataset['point'].astype(str)\n",
    "# dataset.loc[dataset['point'] == '3', 'score'] = 3\n",
    "# dataset.loc[dataset['point'] == '2A', 'score'] = 2\n",
    "# dataset.loc[dataset['point'] == '2B', 'score'] = 1\n",
    "# dataset.loc[dataset['point'] == '2C', 'score'] = 0\n",
    "# print(dataset['score'].max())\n",
    "\n",
    "# # Nanのものを削除\n",
    "# dataset.dropna(subset=['score'], inplace=True)\n",
    "# # scoreの行をintにする\n",
    "# dataset['score'] = dataset['score'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2,   4,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  43,  44,  45,  46,  47,  48,  49,  50,\n",
       "        51,  52,  53,  54,  55,  56,  57,  58,  70,  71,  72,  73,  74,\n",
       "        75,  76,  77,  78,  79,  80,  81,  82,  83,  84,  85,  86,  99,\n",
       "       100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
       "       113, 114, 115, 116], dtype=int64)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['uid'].unique()\n",
    "# dataset['date'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grasp動作だけを抜き出す\n",
    "# dataset_grasp = dataset[dataset['task_id'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RElbow2RWrist と Rshoulder2RElbow と Pelvis2RShoulder と Pelvis2Neck の速度を入れてみる "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_annotate\n",
    "# l_date = df_annotate['date'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_vector(df, joint1, joint2, word=\"\"):\n",
    "    df[f\"{word}_x_{joint1}2{joint2}\"] = df[f\"x_{joint1}\"].astype('float64') - df[f\"x_{joint2}\"].astype('float64')\n",
    "    df[f\"{word}_y_{joint1}2{joint2}\"] = df[f\"y_{joint1}\"].astype('float64') - df[f\"y_{joint2}\"].astype('float64')\n",
    "    df[f\"{word}_z_{joint1}2{joint2}\"] = df[f\"z_{joint1}\"].astype('float64') - df[f\"z_{joint2}\"].astype('float64')\n",
    "    df[f\"{word}_ave_{joint1}2{joint2}\"] = (df[f\"{word}_x_{joint1}2{joint2}\"] + df[f\"{word}_y_{joint1}2{joint2}\"] + df[f\"{word}_z_{joint1}2{joint2}\"])/3\n",
    "    return df\n",
    "\n",
    "def calc_angle(df, joint1, joint2, joint3):\n",
    "    l_theta = []\n",
    "    # 角度を算出する関数。p2を支点として、他の二点間の角度を算出する。\n",
    "    x1, y1 ,z1= df[f\"x_{joint1}\"].astype('float64'), df[f\"y_{joint1}\"].astype('float64'), df[f\"z_{joint1}\"].astype('float64')\n",
    "    x2, y2 ,z2= df[f\"x_{joint2}\"].astype('float64'), df[f\"y_{joint2}\"].astype('float64'), df[f\"z_{joint2}\"].astype('float64')\n",
    "    x3, y3 ,z3= df[f\"x_{joint3}\"].astype('float64'), df[f\"y_{joint3}\"].astype('float64'), df[f\"z_{joint3}\"].astype('float64')\n",
    "    v1 = np.array([x1-x2, y1-y2, z1-z2])\n",
    "    v2 = np.array([x3-x2, y3-y2, z3-z2])\n",
    "    for i in range(v1.shape[1]):\n",
    "        cos = np.dot(v1[:,i], v2[:,i])/(np.linalg.norm(v1[:,i], ord=2) * np.linalg.norm(v2[:,i], ord=2))\n",
    "        theta = np.degrees(math.acos(cos))\n",
    "        l_theta.append(theta)\n",
    "    return np.array(l_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 角度と角度の速度を入力とする\n",
    "angle_RElbow = calc_angle(dataset, 'RShoulder', 'RElbow', 'RWrist')\n",
    "diff_angle_RElbow = np.diff(angle_RElbow)\n",
    "\n",
    "angle_RShoulder = calc_angle(dataset, 'RElbow', 'RShoulder', 'Neck')\n",
    "diff_angle_RShoulder = np.diff(angle_RShoulder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l_date_name = np.array(dataset['date'].unique())\n",
    "# l = []\n",
    "# for date in l_date_name:\n",
    "#     score = dataset[dataset[\"date\"] == date]['score'].mode()\n",
    "#     l.append(score)\n",
    "# return np.array(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正解ラベルで用いるyのone hotベクトルを作成\n",
    "# def extract_y(df, augment_flag=False, rate_augment=10):\n",
    "#     l = []\n",
    "#     l_date_name = np.array(df['date'].unique())\n",
    "#     for date in l_date_name:\n",
    "#         score = df[df[\"date\"] == date]['score'].mode()\n",
    "#         l.append(score)\n",
    "#         if augment_flag:\n",
    "#             for i in range(rate_augment):\n",
    "#                 l.append(score)\n",
    "#     y = np.array(l).flatten()\n",
    "    \n",
    "#     # l_date_name = np.array(df['date'].unique())\n",
    "#     # y = np.array(df[\"score\"]) #一番多いものをscoreとして最後にyとして返す\n",
    "#     # y_arr = np.ones(l_date_name.shape)\n",
    "#     # y_arr = y_arr * y\n",
    "#     return y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def interpolate1(array, length=100):\n",
    "    x_old = np.linspace(0, 1, array.shape[0])\n",
    "    y_old = array\n",
    "    \n",
    "    f = interpolate.interp1d(x_old, y_old)\n",
    "\n",
    "    x = np.linspace(0, 1, length)\n",
    "    y_new = f(x)\n",
    "    return y_new\n",
    "\n",
    "def zscore(x, axis = 1):\n",
    "    xmean = x.mean(axis=axis, keepdims=True)\n",
    "    xstd  = np.std(x, axis=axis, keepdims=True)\n",
    "    zscore = (x-xmean)/xstd\n",
    "    return zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xの系列を入力とすると，rateのものを10倍作成して返す\n",
    "def augmentation(arr_data, rate=0.8, num_arr=10, origin_length=100):\n",
    "    #まず100個に補間 shape:(100, num_feature)\n",
    "    interpolate = interpolate_one_sample(arr_data)\n",
    "    interpolate = interpolate.reshape(interpolate.shape[1], -1)\n",
    "    \n",
    "    num_data = origin_length * rate #実際に用いるデータ点列\n",
    "    \n",
    "    for i in range(num_arr):\n",
    "        first_idx = round( i/num_arr * origin_length * (1-rate))\n",
    "        end_idx = round( i/num_arr * origin_length * (1-rate) + origin_length * rate )\n",
    "        # print(f\"first index is {first_idx}, end index is {end_idx}\")\n",
    "        one_trimmed = interpolate[first_idx:end_idx, :]\n",
    "        one_arr = interpolate_one_sample(one_trimmed)\n",
    "        if i == 0:\n",
    "                x = one_arr #(1, 100, num_feature)\n",
    "        else:\n",
    "            x = np.concatenate([x, one_arr],0)\n",
    "    return x # (num_arr, 100, num_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_one_sample(arr_data):\n",
    "    num_feature = arr_data.shape[1]\n",
    "    for i in range(num_feature):\n",
    "            tmp_interpolate = interpolate1(arr_data[:, i])\n",
    "            #3次元に拡張\n",
    "            origin = tmp_interpolate.reshape(1, tmp_interpolate.shape[0], 1)\n",
    "            \n",
    "            if i == 0:\n",
    "                x = origin\n",
    "            else:\n",
    "                # print(f\"x shape is {x.shape}, \")\n",
    "                x = np.concatenate([x, origin],2)\n",
    "    return x #(1, 100, num_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# この関数を実行しさえすればデータセットが作成される\n",
    "def extract_y(df, augment_flag=False, rate_augment=10, label=\"score\"):\n",
    "    l = []\n",
    "    l_date_name = np.array(df['date'].unique())\n",
    "    for date in l_date_name:\n",
    "        score = df[df[\"date\"] == date][\"shoulder\"].mode()\n",
    "        l.append(score)\n",
    "        if augment_flag:\n",
    "            for i in range(rate_augment):\n",
    "                l.append(score)\n",
    "    y = np.array(l).flatten()\n",
    "    return y\n",
    "\n",
    "\n",
    "def create_compensate_Xy(dataset, augment_flag=False, rate_augment=5, rate=0.9):\n",
    "    #正解ラベルの作成\n",
    "    y_shoulder = extract_y(dataset, augment_flag, rate_augment=rate_augment, label=\"shoulder\")\n",
    "    y_body = extract_y(dataset, augment_flag, rate_augment=rate_augment, label=\"body\")\n",
    "    \n",
    "    # Xの作成\n",
    "    l_date_name = np.array(dataset['date'].unique())\n",
    "    for k, date in enumerate(l_date_name):\n",
    "        #一連のデータ，1サンプル\n",
    "        series = dataset[dataset['date'] == l_date_name[k]]\n",
    "        series.drop(columns='point', inplace=True)\n",
    "        series.drop(columns='date', inplace=True)\n",
    "        #差分ベクトルを取得\n",
    "        diff = series.diff()\n",
    "        diff.fillna(0, inplace=True)\n",
    "\n",
    "        # 間接点同士のベクトルのカラムをデータフレームに追加\n",
    "        # これらがデータセットになる\n",
    "        diff = add_vector(diff, 'RElbow', \"RWrist\", 'v')\n",
    "        diff = add_vector(diff, 'RShoulder', \"RElbow\", 'v')\n",
    "        diff = add_vector(diff, 'Pelvis', \"RShoulder\", 'v')\n",
    "        diff = add_vector(diff, 'Pelvis', \"Neck\", 'v')\n",
    "        diff = add_vector(diff, 'Neck', 'RShoulder', 'v')\n",
    "        diff = add_vector(diff, 'Neck', 'LShoulder', 'v')\n",
    "        \n",
    "        # 1サンプルに関するデータ shape = (num_timestep, num_feature)\n",
    "        arr_data = diff.loc[:, \"v_x_RElbow2RWrist\":].values\n",
    "        \n",
    "        origin = interpolate_one_sample(arr_data)\n",
    "        \n",
    "        # augmentで増やす\n",
    "        # この関数の出力 (augmented_num, 100, num_feature)\n",
    "        if augment_flag:\n",
    "            augmented_arr = augmentation(arr_data, rate=rate, num_arr=rate_augment)\n",
    "            augmented_origin_arr = np.concatenate([origin, augmented_arr], 0)\n",
    "        else:\n",
    "            augmented_origin_arr = origin\n",
    "                \n",
    "        #最初だけ条件分岐\n",
    "        if k == 0:\n",
    "            out = augmented_origin_arr\n",
    "        else:\n",
    "            out = np.concatenate([out, augmented_origin_arr], axis=0)     \n",
    "    # 最後にzscore変換\n",
    "        # print(X.shape)\n",
    "    X = zscore(out)\n",
    "    return X, y_shoulder, y_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# この関数を実行しさえすればデータセットが作成される\n",
    "def create_Xy(dataset, augment_flag=False, rate_augment=5, rate=0.9):\n",
    "    #正解ラベルの作成\n",
    "    y = extract_y(dataset, augment_flag, rate_augment=rate_augment)\n",
    "    X = None\n",
    "    \n",
    "    # Xの作成\n",
    "    l_date_name = np.array(dataset['date'].unique())\n",
    "    for k, date in enumerate(l_date_name):\n",
    "        #一連のデータ，1サンプル\n",
    "        series = dataset[dataset['date'] == l_date_name[k]]\n",
    "        series.drop(columns='point', inplace=True)\n",
    "        series.drop(columns='date', inplace=True)\n",
    "        #差分ベクトルを取得\n",
    "        diff = series.diff()\n",
    "        diff.fillna(0, inplace=True)\n",
    "\n",
    "        # 間接点同士のベクトルのカラムをデータフレームに追加\n",
    "        # これらがデータセットになる\n",
    "        diff = add_vector(diff, 'RElbow', \"RWrist\", 'v')\n",
    "        diff = add_vector(diff, 'RShoulder', \"RElbow\", 'v')\n",
    "        diff = add_vector(diff, 'Pelvis', \"RShoulder\", 'v')\n",
    "        diff = add_vector(diff, 'Pelvis', \"Neck\", 'v')\n",
    "        diff = add_vector(diff, 'Neck', 'RShoulder', 'v')\n",
    "        diff = add_vector(diff, 'Neck', 'LShoulder', 'v')\n",
    "        \n",
    "        # 1サンプルに関するデータ shape = (num_timestep, num_feature)\n",
    "        arr_data = diff.loc[:, \"v_x_RElbow2RWrist\":].values\n",
    "        \n",
    "        origin = interpolate_one_sample(arr_data)\n",
    "        \n",
    "        # augmentで増やす\n",
    "        # この関数の出力 (augmented_num, 100, num_feature)\n",
    "        if augment_flag:\n",
    "            augmented_arr = augmentation(arr_data, rate=rate, num_arr=rate_augment)\n",
    "            augmented_origin_arr = np.concatenate([origin, augmented_arr], 0)\n",
    "        else:\n",
    "            augmented_origin_arr = origin\n",
    "                \n",
    "        #最初だけ条件分岐\n",
    "        if k == 0:\n",
    "            out = augmented_origin_arr\n",
    "        else:\n",
    "            out = np.concatenate([out, augmented_origin_arr], axis=0)     \n",
    "    # 最後にzscore変換\n",
    "        # print(X.shape)\n",
    "    X = zscore(out)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ここが違う！ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxW659H6YTpz"
   },
   "source": [
    "## Build the model\n",
    "\n",
    "Our model processes a tensor of shape `(batch size, sequence length, features)`,\n",
    "where `sequence length` is the number of time steps and `features` is each input\n",
    "timeseries.\n",
    "\n",
    "You can replace your classification RNN layers with this one: the\n",
    "inputs are fully compatible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urNK4ym-YTp0"
   },
   "source": [
    "We include residual connections, layer normalization, and dropout.\n",
    "The resulting layer can be stacked multiple times.\n",
    "\n",
    "The projection layers are implemented through `keras.layers.Conv1D`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    '''入力された単語の位置を示すベクトル情報を付加する'''\n",
    "\n",
    "    def __init__(self, d_model=12, max_seq_len=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model  # 単語ベクトルの次元数\n",
    "\n",
    "        # 単語の順番（pos）と埋め込みベクトルの次元の位置（i）によって一意に定まる値の表をpeとして作成\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "\n",
    "        # GPUが使える場合はGPUへ送る、ここでは省略。実際に学習時には使用する\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        pe = pe.to(device)\n",
    "\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                \n",
    "                # 誤植修正_200510 #79\n",
    "                # pe[pos, i + 1] = math.cos(pos /\n",
    "                #                          (10000 ** ((2 * (i + 1))/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos /\n",
    "                                          (10000 ** ((2 * i)/d_model)))\n",
    "\n",
    "        # 表peの先頭に、ミニバッチ次元となる次元を足す\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "        # 勾配を計算しないようにする\n",
    "        self.pe.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 入力xとPositonal Encodingを足し算する\n",
    "        # xがpeよりも小さいので、大きくする\n",
    "        ret = math.sqrt(self.d_model)*x + self.pe\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # SAGANでは1dConvを使用したが、今回は全結合層で特徴量を変換する\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # 出力時に使用する全結合層\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Attentionの大きさ調整の変数\n",
    "        self.d_k = d_model\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        # 全結合層で特徴量を変換\n",
    "        k = self.k_linear(k)\n",
    "        q = self.q_linear(q)\n",
    "        v = self.v_linear(v)\n",
    "\n",
    "        # Attentionの値を計算する\n",
    "        # 各値を足し算すると大きくなりすぎるので、root(d_k)で割って調整\n",
    "        weights = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # ここでmaskを計算\n",
    "        # mask = mask.unsqueeze(1)\n",
    "        # print(mask)\n",
    "        # weights = weights.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # softmaxで規格化をする\n",
    "        normlized_weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "        # AttentionをValueとかけ算\n",
    "        output = torch.matmul(normlized_weights, v)\n",
    "\n",
    "        # 全結合層で特徴量を変換\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, normlized_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=1024, dropout=0.1):\n",
    "        '''Attention層から出力を単純に全結合層2つで特徴量を変換するだけのユニットです'''\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.dropout(F.relu(x))\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # LayerNormalization層\n",
    "        # https://pytorch.org/docs/stable/nn.html?highlight=layernorm\n",
    "        self.norm_1 = nn.LayerNorm(d_model)\n",
    "        self.norm_2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Attention層\n",
    "        self.attn = Attention(d_model)\n",
    "\n",
    "        # Attentionのあとの全結合層2つ\n",
    "        self.ff = FeedForward(d_model)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # 正規化とAttention\n",
    "        # print(f\"input shape is {x.shape}\")\n",
    "        x_normlized = self.norm_1(x)\n",
    "        output, attn_weights = self.attn(x_normlized, x_normlized, x_normlized, mask)\n",
    "        \n",
    "        x2 = x + self.dropout_1(output)\n",
    "\n",
    "        # 正規化と全結合層\n",
    "        x_normlized2 = self.norm_2(x2)\n",
    "        output = x2 + self.dropout_2(self.ff(x_normlized2))\n",
    "        # print(f\"output shape is {output.shape}\")\n",
    "\n",
    "        return output, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoringHead(nn.Module):\n",
    "    '''Transformer_Blockの出力を使用して，最後にスコアリングを行う'''\n",
    "    \n",
    "    def __init__(self, d_model=300, output_dim=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # 全結合層\n",
    "        self.linear = nn.Linear(d_model, output_dim)  # output_dimはポジ・ネガの2つ\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # 重み初期化処理\n",
    "        nn.init.normal_(self.linear.weight, std=0.02)\n",
    "        nn.init.normal_(self.linear.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = x[:, 0, :]  # 各ミニバッチの各文の先頭の単語の特徴量（300次元）を取り出す\n",
    "        weights = self.linear.weight\n",
    "        out = self.linear(x0)\n",
    "        \n",
    "        # print(f\"Scoring input shape is {x0.shape}\")\n",
    "        # print(f\"Scoring weight shape is {weights.shape}\")\n",
    "        # print(f\"Scoring output shape is {out.shape}\")\n",
    "\n",
    "        return out, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ClassificationHead(nn.Module):\n",
    "#     '''Transformer_Blockの出力を使用し、最後にクラス分類させる'''\n",
    "\n",
    "#     def __init__(self, d_model=300, output_dim=4):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # 全結合層\n",
    "#         self.linear = nn.Linear(d_model, output_dim)  # output_dimはポジ・ネガの2つ\n",
    "#         self.output_dim = output_dim\n",
    "\n",
    "#         # 重み初期化処理\n",
    "#         nn.init.normal_(self.linear.weight, std=0.02)\n",
    "#         nn.init.normal_(self.linear.bias, 0)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x0 = x[:, 0, :]  # 各ミニバッチの各文の先頭の単語の特徴量（300次元）を取り出す\n",
    "#         x1 = self.linear(x0)\n",
    "#         # print(x1.shape)\n",
    "#         out = F.softmax(x1, dim=-1)\n",
    "\n",
    "#         return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ここで複数入力のAttentionにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 最終的なTransformerモデルのクラス\n",
    "\n",
    "# class TransformerClassification(nn.Module):\n",
    "#     '''Transformerでクラス分類させる'''\n",
    "\n",
    "#     def __init__(self, d_model=12, max_seq_len=101, output_dim=4):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # モデル構築\n",
    "#         self.net_Positional = PositionalEncoder(d_model=d_model, max_seq_len=max_seq_len)\n",
    "        \n",
    "#         self.net_Attention_1 = TransformerBlock(d_model=4)\n",
    "#         self.net_Attention_2 = TransformerBlock(d_model=4)\n",
    "#         self.net_Attention_3 = TransformerBlock(d_model=4)\n",
    "#         self.net_Attention_4 = TransformerBlock(d_model=4)\n",
    "#         self.net_Attention_5 = TransformerBlock(d_model=4)\n",
    "#         self.net_Attention_6 = TransformerBlock(d_model=4)\n",
    "        \n",
    "#         self.net_Attention_all = TransformerBlock(d_model=24)\n",
    "#         # self.net3_2 = TransformerBlock(d_model=d_model)\n",
    "        \n",
    "#         self.net_Classification = ClassificationHead(output_dim=output_dim, d_model=d_model)\n",
    "\n",
    "#     def forward(self, x, mask):\n",
    "#         x = self.net_Positional(x)  # Positon情報を足し算\n",
    "#         #ここでいくつのAttentionアーキテクチャを利用するか決定する\n",
    "#         # num_attention_block = x.shape[2]/4\n",
    "        \n",
    "#         x1, attn_weights_1 = self.net_Attention_1(x[:,:,0:4], mask[:,:,0:4])  # Self-Attentionで特徴量を変換\n",
    "#         x2, attn_weights_2 = self.net_Attention_2(x[:,:,4:8], mask[:,:,4:8])\n",
    "#         x3, attn_weights_3 = self.net_Attention_3(x[:,:,8:12], mask[:,:,8:12])\n",
    "#         x4, attn_weights_4 = self.net_Attention_4(x[:,:,12:16], mask[:,:,12:16])\n",
    "#         x5, attn_weights_5 = self.net_Attention_5(x[:,:,16:20], mask[:,:,16:20])\n",
    "#         x6, attn_weights_6 = self.net_Attention_6(x[:,:,20:24], mask[:,:,20:24])\n",
    "        \n",
    "#         #出力を全部つなぎ合わせる\n",
    "#         x_concat = torch.cat((x1,x2,x3,x4,x5,x6), 2)\n",
    "#         # x_attn, attn_weights_all = self.net_Attention_all(x_concat, mask)\n",
    "        \n",
    "#         x_out = self.net_Classification(x_concat)  # 最終出力の0単語目を使用して、分類0-1のスカラーを出力\n",
    "        \n",
    "#         l_attn_weights = [attn_weights_1,attn_weights_2,attn_weights_3,attn_weights_4,attn_weights_5,attn_weights_6\n",
    "#                           # , attn_weights_all\n",
    "#                          ] \n",
    "#         return x_out, l_attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 最終的なTransformerモデルのクラス\n",
    "\n",
    "# class TransformerScoring(nn.Module):\n",
    "#     '''Transformerでクラス分類させる'''\n",
    "\n",
    "#     def __init__(self, d_model=12, max_seq_len=101, output_dim=4):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # モデル構築\n",
    "#         self.net_Positional = PositionalEncoder(d_model=d_model, max_seq_len=max_seq_len)\n",
    "        \n",
    "#         self.net_Attention_1 = TransformerBlock(d_model=4)\n",
    "#         self.net_Attention_2 = TransformerBlock(d_model=4)\n",
    "#         self.net_Attention_3 = TransformerBlock(d_model=4)\n",
    "#         self.net_Attention_4 = TransformerBlock(d_model=4)\n",
    "#         self.net_Attention_5 = TransformerBlock(d_model=4)\n",
    "#         self.net_Attention_6 = TransformerBlock(d_model=4)\n",
    "        \n",
    "#         self.net_Attention_all = TransformerBlock(d_model=24)\n",
    "#         # self.net3_2 = TransformerBlock(d_model=d_model)\n",
    "        \n",
    "#         # self.net_Classification = ClassificationHead(output_dim=output_dim, d_model=d_model)\n",
    "#         self.net_Scoring = ScoringHead(output_dim=output_dim, d_model=d_model)\n",
    "\n",
    "#     def forward(self, x, mask):\n",
    "#         x = self.net_Positional(x)  # Positon情報を足し算\n",
    "#         #ここでいくつのAttentionアーキテクチャを利用するか決定する\n",
    "#         # num_attention_block = x.shape[2]/4\n",
    "        \n",
    "#         x1, attn_weights_1 = self.net_Attention_1(x[:,:,0:4], mask[:,:,0:4])  # Self-Attentionで特徴量を変換\n",
    "#         x2, attn_weights_2 = self.net_Attention_2(x[:,:,4:8], mask[:,:,4:8])\n",
    "#         x3, attn_weights_3 = self.net_Attention_3(x[:,:,8:12], mask[:,:,8:12])\n",
    "#         x4, attn_weights_4 = self.net_Attention_4(x[:,:,12:16], mask[:,:,12:16])\n",
    "#         x5, attn_weights_5 = self.net_Attention_5(x[:,:,16:20], mask[:,:,16:20])\n",
    "#         x6, attn_weights_6 = self.net_Attention_6(x[:,:,20:24], mask[:,:,20:24])\n",
    "        \n",
    "#         #出力を全部つなぎ合わせる\n",
    "#         x_concat = torch.cat((x1,x2,x3,x4,x5,x6), 2)\n",
    "#         x_attn, attn_weights_all = self.net_Attention_all(x_concat, mask)\n",
    "#         x_out, _ = self.net_Scoring(x_attn)        \n",
    "#         # x_out, _ = self.net_Scoring(x_concat)  # 最終出力の0単語目を使用して、分類0-1のスカラーを出力\n",
    "        \n",
    "#         l_attn_weights = [attn_weights_1,attn_weights_2,attn_weights_3,attn_weights_4,attn_weights_5,attn_weights_6\n",
    "#                           , attn_weights_all\n",
    "#                          ] \n",
    "#         return x_out, l_attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerScoring(nn.Module):\n",
    "    '''Transformerでクラス分類させる'''\n",
    "\n",
    "    def __init__(self, d_model=12, max_seq_len=101, output_dim=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # モデル構築\n",
    "        self.net_Positional = PositionalEncoder(d_model=d_model, max_seq_len=max_seq_len)\n",
    "        \n",
    "        self.net_Attention_1 = TransformerBlock(d_model=4)\n",
    "        self.net_Attention_2 = TransformerBlock(d_model=4)\n",
    "        self.net_Attention_3 = TransformerBlock(d_model=4)\n",
    "        self.net_Attention_4 = TransformerBlock(d_model=4)\n",
    "        self.net_Attention_5 = TransformerBlock(d_model=4)\n",
    "        self.net_Attention_6 = TransformerBlock(d_model=4)\n",
    "        \n",
    "        self.net_Attention_all = TransformerBlock(d_model=24)\n",
    "        # self.net3_2 = TransformerBlock(d_model=d_model)\n",
    "        \n",
    "        # self.net_Classification = ClassificationHead(output_dim=output_dim, d_model=d_model)\n",
    "        self.net_Scoring1 = ScoringHead(output_dim=output_dim, d_model=d_model)\n",
    "        self.net_Scoring2 = ScoringHead(output_dim=output_dim, d_model=d_model)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.net_Positional(x)  # Positon情報を足し算\n",
    "        #ここでいくつのAttentionアーキテクチャを利用するか決定する\n",
    "        # num_attention_block = x.shape[2]/4\n",
    "        \n",
    "        x1, attn_weights_1 = self.net_Attention_1(x[:,:,0:4], mask[:,:,0:4])  # Self-Attentionで特徴量を変換\n",
    "        x2, attn_weights_2 = self.net_Attention_2(x[:,:,4:8], mask[:,:,4:8])\n",
    "        x3, attn_weights_3 = self.net_Attention_3(x[:,:,8:12], mask[:,:,8:12])\n",
    "        x4, attn_weights_4 = self.net_Attention_4(x[:,:,12:16], mask[:,:,12:16])\n",
    "        x5, attn_weights_5 = self.net_Attention_5(x[:,:,16:20], mask[:,:,16:20])\n",
    "        x6, attn_weights_6 = self.net_Attention_6(x[:,:,20:24], mask[:,:,20:24])\n",
    "        \n",
    "        #出力を全部つなぎ合わせる\n",
    "        x_concat = torch.cat((x1,x2,x3,x4,x5,x6), 2)\n",
    "        # x_attn, attn_weights_all = self.net_Attention_all(x_concat, mask)\n",
    "        \n",
    "        x_shoulder, _ = self.net_Scoring1(x_concat)\n",
    "        x_body, _ = self.net_Scoring2(x_concat)\n",
    "        # x_out, _ = self.net_Scoring(x_concat)  # 最終出力の0単語目を使用して、分類0-1のスカラーを出力\n",
    "        \n",
    "        l_attn_weights = [attn_weights_1,attn_weights_2,attn_weights_3,attn_weights_4,attn_weights_5,attn_weights_6\n",
    "                          # , attn_weights_all\n",
    "                         ] \n",
    "        return x_shoulder, x_body, l_attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# validation用に1人のデータを検証データに回す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = dataset\n",
    "# [~(dataset['subject_id'] == 4)]\n",
    "# val_df = dataset[dataset['subject_id'] == 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PyTorchのdatasetの作成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, y_train = create_Xy(train_df, augment_flag=True, rate_augment=5, rate=0.95)\n",
    "# x_test, y_test = create_Xy(val_df, augment_flag=True, rate_augment=5, rate=0.95)\n",
    "\n",
    "# y_train = y_train.astype('int64')\n",
    "# y_test = y_test.astype('int64')\n",
    "\n",
    "\n",
    "# 正解データをone hotベクトルに変換\n",
    "# n_classes = len(np.unique(y_train))\n",
    "# y_train_onehot = np.identity(n_classes)[y_train]\n",
    "# y_test_onehot = np.identity(n_classes)[y_test]\n",
    "\n",
    "\n",
    "# ### 先頭にclassification用のトークンを追加する\n",
    "# score_train = np.zeros((x_train.shape[0], 1 , x_train.shape[2]))\n",
    "# x_train = np.concatenate([score_train, x_train], axis=1)\n",
    "\n",
    "# score_test = np.zeros((x_test.shape[0], 1 , x_test.shape[2]))\n",
    "# x_test = np.concatenate([score_test, x_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 先頭にclassification用のトークンを追加する\n",
    "# score_train = np.zeros((x_train.shape[0], 1 , x_train.shape[2]))\n",
    "# x_train = np.concatenate([score_train, x_train], axis=1)\n",
    "\n",
    "# score_test = np.zeros((x_test.shape[0], 1 , x_test.shape[2]))\n",
    "# x_test = np.concatenate([score_test, x_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LakZIHM8YTp1"
   },
   "source": [
    "## Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 動作確認\n",
    "# # ミニバッチの用意\n",
    "# batch = next(iter(train_dataloader))\n",
    "\n",
    "# # モデル構築\n",
    "\n",
    "# # 変数の固定\n",
    "# torch.manual_seed(1234)\n",
    "# torch.cuda.manual_seed(1234)\n",
    "\n",
    "# net = TransformerScoring(\n",
    "#     d_model=24, max_seq_len=101, output_dim=1)\n",
    "\n",
    "# # 入出力\n",
    "# _device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# net.to(_device)\n",
    "# x = batch[0].to(_device)\n",
    "# input_pad = 1\n",
    "# input_mask = (x != input_pad)\n",
    "# out, l_weights = net(x, input_mask)\n",
    "\n",
    "# print(\"出力のテンソルサイズ：\", out.shape)\n",
    "# # print(\"出力テンソル\", out)\n",
    "# # print(\"出力テンソルのsigmoid：\", F.softmax(out, dim=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elTTH_9KYTp0"
   },
   "source": [
    "The main part of our model is now complete. We can stack multiple of those\n",
    "`transformer_encoder` blocks and we can also proceed to add the final\n",
    "Multi-Layer Perceptron classification head. Apart from a stack of `Dense`\n",
    "layers, we need to reduce the output tensor of the `TransformerEncoder` part of\n",
    "our model down to a vector of features for each data point in the current\n",
    "batch. A common way to achieve this is to use a pooling layer. For\n",
    "this example, a `GlobalAveragePooling1D` layer is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ネットワークの初期化を定義\n",
    "\n",
    "# def weights_init(m):\n",
    "#     classname = m.__class__.__name__\n",
    "#     if classname.find('Linear') != -1:\n",
    "#         # Liner層の初期化\n",
    "#         nn.init.kaiming_normal_(m.weight)\n",
    "#         if m.bias is not None:\n",
    "#             nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "# # 訓練モードに設定\n",
    "# net.train()\n",
    "\n",
    "# # TransformerBlockモジュールを初期化実行\n",
    "\n",
    "# net.net_Attention_1.apply(weights_init)\n",
    "\n",
    "\n",
    "\n",
    "# print('ネットワーク設定完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数の設定\n",
    "# # criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.MSELoss()\n",
    "# # nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算\n",
    "\n",
    "# # 最適化手法の設定\n",
    "# learning_rate = 3e-4\n",
    "# optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs, patience, label, loss_rate=0.5):\n",
    "    # Early Stopping を定義\n",
    "    early_stopping = EarlyStopping(patience=patience, label=label)\n",
    "\n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"使用デバイス：\", device)\n",
    "    print('-----start-------')\n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # epochのループ\n",
    "    flag = False\n",
    "    for epoch in range(num_epochs):\n",
    "        # epochごとの訓練と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "        # for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "            else:\n",
    "                net.eval()   # モデルを検証モードに\n",
    "\n",
    "            epoch_loss = 0.0  # epochの損失和\n",
    "            epoch_corrects_shoulder = 0 # epochの正解数\n",
    "            epoch_corrects_body = 0\n",
    "\n",
    "            # データローダーからミニバッチを取り出すループ\n",
    "            for batch in (dataloaders_dict[phase]):\n",
    "                # batchはTextとLableの辞書オブジェクト\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                inputs = batch[0].to(device)  # 文章\n",
    "                labels_shoulder = batch[1].to(device)  # 肩ラベル\n",
    "                labels_body = batch[2].to(device)  # 肩ラベル\n",
    "\n",
    "                # optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                    # mask作成\n",
    "                    input_pad = 1  # 単語のIDにおいて、'<pad>': 1 なので\n",
    "                    input_mask = (inputs != input_pad)\n",
    "\n",
    "                    # Transformerに入力\n",
    "                    outputs_shoulder, outputs_body, _ = net(inputs, input_mask)\n",
    "                    outputs_shoulder = outputs_shoulder.flatten()\n",
    "                    outputs_body = outputs_body.flatten()\n",
    "                    loss_shoulder = loss_rate*criterion(outputs_shoulder, labels_shoulder.float())\n",
    "                    loss_body =  (1-loss_rate)*criterion(outputs_body, labels_body.float())\n",
    "                    loss = loss_shoulder + loss_body # 損失を計算\n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    # 結果の計算\n",
    "                    epoch_loss += loss.item() * inputs.size(0)  # lossの合計を更新\n",
    "                    # 正解数の合計を更新\n",
    "                    \n",
    "                    preds_shoulder = torch.round(outputs_shoulder)  # ラベルを予測\n",
    "                    preds_body = torch.round(outputs_body)  # ラベルを予測\n",
    "                    \n",
    "                    epoch_corrects_shoulder += torch.sum(preds_shoulder == labels_shoulder)\n",
    "                    epoch_corrects_body += torch.sum(preds_body == labels_body)\n",
    "\n",
    "            # epochごとのlossと正解率\n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "            epoch_acc_shoulder = epoch_corrects_shoulder.double() / len(dataloaders_dict[phase].dataset)\n",
    "            epoch_acc_body = epoch_corrects_body.double() / len(dataloaders_dict[phase].dataset)\n",
    "            \n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc S: {:.4f} Acc B: {:.4f}'.format(epoch+1, num_epochs,\n",
    "                                                                           phase, epoch_loss, epoch_acc_shoulder, epoch_acc_body))\n",
    "                # print(f\"epoch corrects {epoch_corrects.double()}\")\n",
    "                # print(f\"dataset size = {len(dataloaders_dict[phase].dataset)}\")\n",
    "                \n",
    "            if phase == 'train':\n",
    "                # writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "                writer.add_scalar(\"Loss/train/shoulder\", loss_shoulder, epoch)\n",
    "                writer.add_scalar(\"Loss/train/body\", loss_shoulder, epoch)\n",
    "\n",
    "            else:\n",
    "                # writer.add_scalar(\"Loss/test\", loss, epoch)\n",
    "                writer.add_scalar(\"Loss/test/shoulder\", loss_shoulder, epoch)\n",
    "                writer.add_scalar(\"Loss/test/body\", loss_shoulder, epoch)\n",
    "                early_stopping(loss, net) # 最良モデルならモデルパラメータ保存\n",
    "                if early_stopping.early_stop: \n",
    "                # 一定epochだけval_lossが最低値を更新しなかった場合、ここに入り学習を終了\n",
    "                    print(f\"Early Stopping!! Epoch {epoch+1}/{num_epochs} Loss: {epoch_loss:.4f} Acc: S={epoch_acc_shoulder:.4f}, B={epoch_acc_body:.4f} :: {epoch_corrects_shoulder.double()}/{len(dataloaders_dict[phase].dataset)}, {epoch_corrects_body.double()}/{len(dataloaders_dict[phase].dataset)}\")\n",
    "                    flag = True\n",
    "        if flag:\n",
    "            break\n",
    "\n",
    "    return net, epoch_loss, epoch_acc_shoulder+epoch_acc_body\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #損失関数\n",
    "# criterion = nn.MSELoss()\n",
    "# num_epochs = 1000\n",
    "# patience = 100\n",
    "# batch_size = 32\n",
    "\n",
    "# n_classes = 4\n",
    "\n",
    "# # グリッドサーチを行う．\n",
    "# l_rate = [1e-5, 1e-4, 1e-3, 1e-2]\n",
    "# # l_rate = [1e-3]\n",
    "\n",
    "# seeds = [1000]\n",
    "# seed = 1000\n",
    "# # for i in range(15):\n",
    "# #     seeds.append(1000+i)\n",
    "\n",
    "\n",
    "# for seed in seeds:\n",
    "#     print(\"---------------------------------------------------------------------------------\")\n",
    "#     print(f\"seed is {seed}\")\n",
    "#     # 変数の固定\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "    \n",
    "#     for learning_rate in l_rate:\n",
    "\n",
    "#         # kf = KFold(n_splits=5, shuffle=True, random_state=2022)\n",
    "#         cv_loss = 0\n",
    "#         cv_acc = 0\n",
    "\n",
    "#         for subject_id in range(n_classes): \n",
    "#             print(\"---------------------------------------------------------------------------------\")\n",
    "#             print(f\"test subject id : {subject_id+1}\")\n",
    "#             train_df = dataset[~(dataset['subject_id'] == (subject_id+1))]\n",
    "#             val_df = dataset[dataset['subject_id'] == (subject_id+1)]\n",
    "\n",
    "#             x_train, y_train = create_Xy(train_df, augment_flag=True, rate_augment=5, rate=0.95)\n",
    "#             x_test, y_test = create_Xy(val_df, augment_flag=True, rate_augment=5, rate=0.95)\n",
    "\n",
    "#             ### 先頭にclassification用のトークンを追加する\n",
    "#             score_train = np.zeros((x_train.shape[0], 1 , x_train.shape[2]))\n",
    "#             x_train = np.concatenate([score_train, x_train], axis=1)\n",
    "#             score_test = np.zeros((x_test.shape[0], 1 , x_test.shape[2]))\n",
    "#             x_test = np.concatenate([score_test, x_test], axis=1)\n",
    "#             y_train_onehot = np.identity(n_classes)[y_train]\n",
    "#             y_test_onehot = np.identity(n_classes)[y_test]\n",
    "\n",
    "#             train_dataset = torch.utils.data.TensorDataset(torch.tensor(x_train, dtype=torch.float32)\n",
    "#                                                        , torch.tensor(y_train_onehot, dtype=torch.int8))\n",
    "#             val_dataset = torch.utils.data.TensorDataset(torch.tensor(x_test, dtype=torch.float32),\n",
    "#                                                          torch.tensor(y_test_onehot, dtype=torch.int8))\n",
    "#             train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "#             val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "#             dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
    "\n",
    "#             writer = SummaryWriter()\n",
    "#             print(f\"learning_rate is {learning_rate}\")\n",
    "\n",
    "#             # モデル構築\n",
    "#             net = TransformerScoring(d_model=24, max_seq_len=101, output_dim=1)\n",
    "#             # 訓練モードに設定\n",
    "#             net.train()\n",
    "#             optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "#             start = time.time() #時間計測スタート\n",
    "#             net_trained, loss, acc= train_model(net, \n",
    "#                                       dataloaders_dict,\n",
    "#                                       criterion,\n",
    "#                                       optimizer,\n",
    "#                                       num_epochs,\n",
    "#                                       patience\n",
    "#                                      )\n",
    "#             print(time.time() - start)\n",
    "#             writer.flush()\n",
    "#             cv_loss += loss / n_classes\n",
    "#             cv_acc += acc /n_classes\n",
    "#         print(f\"cv_acc is {cv_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suzuk\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "seed is 1002\n",
      "learning_rate is 0.0001\n",
      "使用デバイス： cuda:0\n",
      "-----start-------\n",
      "Epoch 1/1000 | train |  Loss: 6.2746 Acc S: 0.0000 Acc B: 0.0000\n",
      "Epoch 1/1000 |  val  |  Loss: 8.3986 Acc S: 0.0000 Acc B: 0.0000\n",
      "Epoch 101/1000 | train |  Loss: 0.5754 Acc S: 0.4545 Acc B: 0.4515\n",
      "Epoch 101/1000 |  val  |  Loss: 0.7170 Acc S: 0.3571 Acc B: 0.3571\n",
      "Epoch 201/1000 | train |  Loss: 0.4259 Acc S: 0.5030 Acc B: 0.5000\n",
      "Epoch 201/1000 |  val  |  Loss: 0.7538 Acc S: 0.3571 Acc B: 0.3571\n",
      "Early Stopping!! Epoch 272/1000 Loss: 0.8295 Acc: S=0.3452, B=0.3571 :: 29.0/84, 30.0/84\n",
      "75.34761953353882\n",
      "seed is 1002\n",
      "learning_rate is 0.0001\n",
      "使用デバイス： cuda:0\n",
      "-----start-------\n",
      "Epoch 1/1000 | train |  Loss: 2.2479 Acc S: 0.0515 Acc B: 0.3273\n",
      "Epoch 1/1000 |  val  |  Loss: 1.5865 Acc S: 0.4286 Acc B: 0.4286\n",
      "Epoch 101/1000 | train |  Loss: 0.4733 Acc S: 0.4727 Acc B: 0.4606\n",
      "Epoch 101/1000 |  val  |  Loss: 0.6409 Acc S: 0.4881 Acc B: 0.4405\n",
      "Epoch 201/1000 | train |  Loss: 0.3645 Acc S: 0.6000 Acc B: 0.5848\n",
      "Epoch 201/1000 |  val  |  Loss: 0.7351 Acc S: 0.4405 Acc B: 0.4405\n",
      "Epoch 301/1000 | train |  Loss: 0.2683 Acc S: 0.6848 Acc B: 0.7000\n",
      "Epoch 301/1000 |  val  |  Loss: 0.7773 Acc S: 0.4524 Acc B: 0.4048\n",
      "Epoch 401/1000 | train |  Loss: 0.2404 Acc S: 0.6758 Acc B: 0.7030\n",
      "Epoch 401/1000 |  val  |  Loss: 0.7897 Acc S: 0.4286 Acc B: 0.3690\n",
      "Epoch 501/1000 | train |  Loss: 0.1931 Acc S: 0.7576 Acc B: 0.7697\n",
      "Epoch 501/1000 |  val  |  Loss: 0.7931 Acc S: 0.4286 Acc B: 0.4048\n",
      "Epoch 601/1000 | train |  Loss: 0.1800 Acc S: 0.7697 Acc B: 0.7576\n",
      "Epoch 601/1000 |  val  |  Loss: 0.7271 Acc S: 0.4286 Acc B: 0.4286\n",
      "Early Stopping!! Epoch 612/1000 Loss: 0.6827 Acc: S=0.4286, B=0.4286 :: 36.0/84, 36.0/84\n",
      "170.43675136566162\n",
      "seed is 1002\n",
      "learning_rate is 0.0001\n",
      "使用デバイス： cuda:0\n",
      "-----start-------\n",
      "Epoch 1/1000 | train |  Loss: 0.9632 Acc S: 0.3273 Acc B: 0.3636\n",
      "Epoch 1/1000 |  val  |  Loss: 0.8749 Acc S: 0.4286 Acc B: 0.2857\n",
      "Epoch 101/1000 | train |  Loss: 0.5287 Acc S: 0.4182 Acc B: 0.4152\n",
      "Epoch 101/1000 |  val  |  Loss: 0.8302 Acc S: 0.2857 Acc B: 0.2857\n",
      "Early Stopping!! Epoch 106/1000 Loss: 0.8430 Acc: S=0.2857, B=0.2857 :: 24.0/84, 24.0/84\n",
      "29.43622899055481\n",
      "seed is 1002\n",
      "learning_rate is 0.0001\n",
      "使用デバイス： cuda:0\n",
      "-----start-------\n",
      "Epoch 1/1000 | train |  Loss: 0.8722 Acc S: 0.3818 Acc B: 0.3273\n",
      "Epoch 1/1000 |  val  |  Loss: 0.9554 Acc S: 0.2143 Acc B: 0.4286\n",
      "Epoch 101/1000 | train |  Loss: 0.4509 Acc S: 0.4485 Acc B: 0.4545\n",
      "Epoch 101/1000 |  val  |  Loss: 0.7846 Acc S: 0.2143 Acc B: 0.2143\n",
      "Epoch 201/1000 | train |  Loss: 0.2613 Acc S: 0.6394 Acc B: 0.6545\n",
      "Epoch 201/1000 |  val  |  Loss: 0.9764 Acc S: 0.2857 Acc B: 0.3095\n",
      "Early Stopping!! Epoch 235/1000 Loss: 1.0524 Acc: S=0.3333, B=0.3810 :: 28.0/84, 32.0/84\n",
      "68.90735292434692\n",
      "seed is 1002\n",
      "learning_rate is 0.0001\n",
      "使用デバイス： cuda:0\n",
      "-----start-------\n",
      "Epoch 1/1000 | train |  Loss: 1.4522 Acc S: 0.3571 Acc B: 0.3274\n",
      "Epoch 1/1000 |  val  |  Loss: 0.9046 Acc S: 0.3077 Acc B: 0.5385\n",
      "Epoch 101/1000 | train |  Loss: 0.5645 Acc S: 0.3958 Acc B: 0.4048\n",
      "Epoch 101/1000 |  val  |  Loss: 0.5464 Acc S: 0.5385 Acc B: 0.5385\n",
      "Early Stopping!! Epoch 157/1000 Loss: 0.7147 Acc: S=0.3205, B=0.3333 :: 25.0/78, 26.0/78\n",
      "43.54766345024109\n",
      "k fold accuracy:0.3499084249084249\n"
     ]
    }
   ],
   "source": [
    "# とりあえずvalidationがうまく動くかを確認\n",
    "\n",
    "uid = dataset[\"uid\"].unique()\n",
    "x_all, y_all,_ = create_compensate_Xy(dataset, augment_flag=False)\n",
    "n_classes = len(np.unique(y_all))\n",
    "\n",
    "#損失関数\n",
    "criterion = nn.MSELoss()\n",
    "num_epochs = 1000\n",
    "patience = 100\n",
    "batch_size = 32\n",
    "\n",
    "# グリッドサーチを行う．\n",
    "l_rate = [1e-4]\n",
    "# l_rate = [1e-3]\n",
    "\n",
    "seeds = [1002, 1004, 1005, 1006, 1007, 1008, 1009, 1010]\n",
    "seeds = [1002]\n",
    "seed = 1000\n",
    "# for i in range(15):\n",
    "#     seeds.append(1000+i)\n",
    "\n",
    "# 変数の固定\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "skf = KFold(n_splits=5, shuffle=True,random_state=2021)\n",
    "\n",
    "for seed in seeds:\n",
    "    \n",
    "    for learning_rate in l_rate:\n",
    "        print(\"---------------------------------------------------------------------------------\")\n",
    "        cv = 0\n",
    "        for _fold, (train_index, test_index) in enumerate(skf.split(x_all, y_all)):  \n",
    "            \n",
    "            train_df = dataset[dataset[\"uid\"].isin(uid[train_index])]\n",
    "            val_df = dataset[dataset[\"uid\"].isin(uid[test_index])]\n",
    "            # print(train_df[\"uid\"].unique())\n",
    "            # print(val_df[\"uid\"].unique())\n",
    "\n",
    "            x_train, y_shoulder_train, y_body_train = create_compensate_Xy(train_df, augment_flag=True, rate_augment=5, rate=0.95)\n",
    "            x_test, y_shoulder_test, y_body_test = create_compensate_Xy(val_df, augment_flag=True, rate_augment=5, rate=0.95)\n",
    "\n",
    "            ### 先頭にclassification用のトークンを追加する\n",
    "            score_train = np.zeros((x_train.shape[0], 1 , x_train.shape[2]))\n",
    "            x_train = np.concatenate([score_train, x_train], axis=1)\n",
    "            score_test = np.zeros((x_test.shape[0], 1 , x_test.shape[2]))\n",
    "            x_test = np.concatenate([score_test, x_test], axis=1)\n",
    "            # y_train_onehot = np.identity(n_classes)[y_train]\n",
    "            # y_test_onehot = np.identity(n_classes)[y_test]\n",
    "\n",
    "            train_dataset = torch.utils.data.TensorDataset(torch.tensor(x_train, dtype=torch.float32)\n",
    "                                                           ,torch.tensor(y_shoulder_train, dtype=torch.int8)\n",
    "                                                            ,torch.tensor(y_body_train, dtype=torch.int8))\n",
    "            val_dataset = torch.utils.data.TensorDataset(torch.tensor(x_test, dtype=torch.float32)\n",
    "                                                         ,torch.tensor(y_shoulder_test, dtype=torch.int8)\n",
    "                                                        ,torch.tensor(y_body_test, dtype=torch.int8))\n",
    "            train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "            val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "            dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
    "            \n",
    "            \n",
    "\n",
    "            writer = SummaryWriter()\n",
    "            print(f\"seed is {seed}\")\n",
    "            print(f\"learning_rate is {learning_rate}\")\n",
    "\n",
    "            # モデル構築\n",
    "            net = TransformerScoring(d_model=24, max_seq_len=101, output_dim=1)\n",
    "            # 訓練モードに設定\n",
    "            net.train()\n",
    "            optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "            start = time.time() #時間計測スタート\n",
    "            net_trained, loss , acc = train_model(net, \n",
    "                                      dataloaders_dict,\n",
    "                                      criterion,\n",
    "                                      optimizer,\n",
    "                                      num_epochs,\n",
    "                                      patience,\n",
    "                                      str(_fold)\n",
    "                                     )\n",
    "            print(time.time() - start)\n",
    "            writer.flush()\n",
    "            cv += acc / (skf.n_splits*2) \n",
    "        # torch.save(model.state_dict(), f\"models/crossvalidation_{_fold}.pth\")\n",
    "        print(f\"k fold accuracy:{cv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# とりあえずvalidationがうまく動くかを確認\n",
    "\n",
    "# uid = dataset[\"uid\"].unique()\n",
    "# x_all, y_all = create_Xy(dataset, augment_flag=False)\n",
    "# n_classes = len(np.unique(y_all))\n",
    "\n",
    "# #損失関数\n",
    "# criterion = nn.MSELoss()\n",
    "# num_epochs = 1000\n",
    "# patience = 100\n",
    "# batch_size = 16\n",
    "\n",
    "# # グリッドサーチを行う．\n",
    "# l_rate = [1e-4, 5e-4, 1e-3]\n",
    "# # l_rate = [1e-3]\n",
    "\n",
    "# # seeds = [1002, 1004, 1005, 1006, 1007, 1008, 1009, 1010]\n",
    "# seeds = [1002]\n",
    "# seed = 1000\n",
    "# # for i in range(15):\n",
    "# #     seeds.append(1000+i)\n",
    "\n",
    "# # 変数の固定\n",
    "# torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=5, shuffle=True,random_state=8888)\n",
    "\n",
    "# for seed in seeds:\n",
    "    \n",
    "#     for learning_rate in l_rate:\n",
    "#         print(\"---------------------------------------------------------------------------------\")\n",
    "#         cv = 0\n",
    "#         for _fold, (train_index, test_index) in enumerate(skf.split(x_all, y_all)):  \n",
    "            \n",
    "#             train_df = dataset[dataset[\"uid\"].isin(uid[train_index])]\n",
    "#             val_df = dataset[dataset[\"uid\"].isin(uid[test_index])]\n",
    "#             # print(train_df[\"uid\"].unique())\n",
    "#             # print(val_df[\"uid\"].unique())\n",
    "\n",
    "#             x_train, y_train = create_Xy(train_df, augment_flag=True, rate_augment=5, rate=0.95)\n",
    "#             x_test, y_test = create_Xy(val_df, augment_flag=True, rate_augment=5, rate=0.95)\n",
    "\n",
    "#             ### 先頭にclassification用のトークンを追加する\n",
    "#             score_train = np.zeros((x_train.shape[0], 1 , x_train.shape[2]))\n",
    "#             x_train = np.concatenate([score_train, x_train], axis=1)\n",
    "#             score_test = np.zeros((x_test.shape[0], 1 , x_test.shape[2]))\n",
    "#             x_test = np.concatenate([score_test, x_test], axis=1)\n",
    "#             y_train_onehot = np.identity(n_classes)[y_train]\n",
    "#             y_test_onehot = np.identity(n_classes)[y_test]\n",
    "\n",
    "#             train_dataset = torch.utils.data.TensorDataset(torch.tensor(x_train, dtype=torch.float32)\n",
    "#                                                        , torch.tensor(y_train_onehot, dtype=torch.int8))\n",
    "#             val_dataset = torch.utils.data.TensorDataset(torch.tensor(x_test, dtype=torch.float32),\n",
    "#                                                          torch.tensor(y_test_onehot, dtype=torch.int8))\n",
    "#             train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "#             val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "#             dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
    "            \n",
    "            \n",
    "\n",
    "#             writer = SummaryWriter()\n",
    "#             print(f\"seed is {seed}\")\n",
    "#             print(f\"learning_rate is {learning_rate}\")\n",
    "\n",
    "#             # モデル構築\n",
    "#             net = TransformerScoring(d_model=24, max_seq_len=101, output_dim=1)\n",
    "#             # 訓練モードに設定\n",
    "#             net.train()\n",
    "#             optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "#             start = time.time() #時間計測スタート\n",
    "#             net_trained, loss , acc = train_model(net, \n",
    "#                                       dataloaders_dict,\n",
    "#                                       criterion,\n",
    "#                                       optimizer,\n",
    "#                                       num_epochs,\n",
    "#                                       patience,\n",
    "#                                       str(_fold)\n",
    "#                                      )\n",
    "#             print(time.time() - start)\n",
    "#             writer.flush()\n",
    "#             cv += acc / skf.n_splits\n",
    "#         # torch.save(model.state_dict(), f\"models/crossvalidation_{_fold}.pth\")\n",
    "#         print(f\"k fold accuracy:{cv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all_df = dataset\n",
    "\n",
    "# x_all, y_all = create_Xy(all_df, augment_flag=False, rate_augment=5, rate=0.95)\n",
    "\n",
    "# # 正解データをone hotベクトルに変換\n",
    "# n_classes = len(np.unique(y_all))\n",
    "# y_all_onehot = np.identity(n_classes)[y_all]\n",
    "\n",
    "# ### 先頭にclassification用のトークンを追加する\n",
    "# score_train = np.zeros((x_all.shape[0], 1 , x_all.shape[2]))\n",
    "# x_all = np.concatenate([score_train, x_all], axis=1)\n",
    "\n",
    "# dataset_all = torch.utils.data.TensorDataset(torch.tensor(x_all, dtype=torch.float32)\n",
    "#                                                , torch.tensor(y_all_onehot, dtype=torch.int8))\n",
    "\n",
    "\n",
    "# #損失関数\n",
    "# criterion = nn.MSELoss()\n",
    "# num_epochs = 1000\n",
    "# patience = 100\n",
    "# batch_size = 8\n",
    "\n",
    "# # グリッドサーチを行う．\n",
    "# l_rate = [5e-4]\n",
    "# # l_rate = [1e-3]\n",
    "\n",
    "# seeds = [1000]\n",
    "# seed = 1000\n",
    "# # for i in range(15):\n",
    "# #     seeds.append(1000+i)\n",
    "\n",
    "# # 変数の固定\n",
    "# torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "\n",
    "# train_index, valid_index = train_test_split(range(len(dataset_all)), test_size=0.2)\n",
    "\n",
    "# for seed in seeds:\n",
    "#     kf = KFold(n_splits=5, shuffle=True, random_state=2022)\n",
    "#     for learning_rate in l_rate:\n",
    "#         print(\"---------------------------------------------------------------------------------\")\n",
    "#         cv = 0\n",
    "#         for _fold, (train_index, test_index) in enumerate(kf.split(dataset_all)):  \n",
    "            \n",
    "#             train_dataset = Subset(dataset_all, train_index)\n",
    "#             train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "#             val_dataset = Subset(dataset_all, valid_index)\n",
    "#             val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "#             dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
    "\n",
    "#             print(f\"seed is {seed}\")\n",
    "\n",
    "#             writer = SummaryWriter()\n",
    "#             print(f\"learning_rate is {learning_rate}\")\n",
    "\n",
    "#             # モデル構築\n",
    "#             net = TransformerScoring(d_model=24, max_seq_len=101, output_dim=1)\n",
    "#             # 訓練モードに設定\n",
    "#             net.train()\n",
    "#             optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "#             start = time.time() #時間計測スタート\n",
    "#             net_trained, loss , acc = train_model(net, \n",
    "#                                       dataloaders_dict,\n",
    "#                                       criterion,\n",
    "#                                       optimizer,\n",
    "#                                       num_epochs,\n",
    "#                                       patience\n",
    "#                                      )\n",
    "#             print(time.time() - start)\n",
    "#             writer.flush()\n",
    "#             cv += acc / kf.n_splits\n",
    "#         print(f\"k fold accuracy:{cv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 学習・検証を実行する 15分ほどかかります\n",
    "# net = TransformerClassification(d_model=24, max_seq_len=101, output_dim=4)\n",
    "# # 訓練モードに設定\n",
    "# net.train()\n",
    "\n",
    "# # 損失関数の設定\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# # nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算\n",
    "\n",
    "# # 最適化手法の設定\n",
    "# learning_rate = 2e-4\n",
    "# optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# print('ネットワーク設定完了')\n",
    "\n",
    "# num_epochs = 300\n",
    "# net_trained = train_model(net, dataloaders_dict,\n",
    "#                           criterion, optimizer, num_epochs=num_epochs)\n",
    "# writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZN2SeFtLHbyT"
   },
   "source": [
    "# SaveModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テストデータ336個での正解率：0.4940 0.4940\n"
     ]
    }
   ],
   "source": [
    "# device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 新しいモデル\n",
    "net_trained.eval()\n",
    "net_trained.to(device)\n",
    "\n",
    "epoch_corrects_shoulder = 0  # epochの正解数\n",
    "epoch_corrects_body = 0\n",
    "\n",
    "# test_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=300)\n",
    "test_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=300)\n",
    "\n",
    "for batch in (test_dataloader):  # testデータのDataLoader\n",
    "    # batchはTextとLableの辞書オブジェクト\n",
    "    \n",
    "    # GPUが使えるならGPUにデータを送る\n",
    "    inputs = batch[0].to(device)  # 文章\n",
    "    labels_shoulder = batch[1].to(device)  # 肩ラベル\n",
    "    labels_body = batch[2].to(device)  # 肩ラベル\n",
    "\n",
    "    # 順伝搬（forward）計算\n",
    "    with torch.set_grad_enabled(False):\n",
    "\n",
    "        # mask作成\n",
    "        input_pad = 1  # 単語のIDにおいて、'<pad>': 1 なので\n",
    "        input_mask = (inputs != input_pad)\n",
    "\n",
    "        # Transformerに入力\n",
    "        outputs_shoulder, outputs_body, _ = net(inputs, input_mask)\n",
    "        outputs_shoulder = outputs_shoulder.flatten()\n",
    "        outputs_body = outputs_body.flatten()\n",
    "        # loss_shoulder = loss_rate*criterion(outputs_shoulder, labels_shoulder.float())\n",
    "        # loss_body =  (1-loss_rate)*criterion(outputs_body, labels_body.float())\n",
    "        # loss = loss_shoulder + loss_body # 損失を計算\n",
    "\n",
    "        # 結果の計算\n",
    "        # epoch_loss += loss.item() * inputs.size(0)  # lossの合計を更新\n",
    "        # 正解数の合計を更新\n",
    "\n",
    "        preds_shoulder = torch.round(outputs_shoulder)  # ラベルを予測\n",
    "        preds_body = torch.round(outputs_body)  # ラベルを予測\n",
    "\n",
    "        epoch_corrects_shoulder += torch.sum(preds_shoulder == labels_shoulder)\n",
    "        epoch_corrects_body += torch.sum(preds_body == labels_body)\n",
    "\n",
    "# 正解率\n",
    "\n",
    "epoch_acc_shoulder = epoch_corrects_shoulder.double() / len(test_dataloader.dataset)\n",
    "epoch_acc_body = epoch_corrects_body.double() / len(test_dataloader.dataset)\n",
    "\n",
    "print('テストデータ{}個での正解率：{:.4f} {:.4f}'.format(len(test_dataloader.dataset),epoch_acc_shoulder, epoch_acc_body))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6644/3842105685.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mpreds_heat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mtrues_heat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# preds_heat[0] = \"3A\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "preds_heat = np.array(preds.cpu())\n",
    "trues_heat = np.array(labels_true.cpu())\n",
    "# preds_heat[0] = \"3A\"\n",
    "# print(preds_heat)\n",
    "# print(trues_heat)\n",
    "\n",
    "heat = confusion_matrix(trues_heat, preds_heat)\n",
    "# print(heat)\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"   # 使用するフォント\n",
    "plt.rcParams[\"font.size\"] = 20        \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "xtics = [\"2C\", \"2B\", \"2A\", \"3\"]\n",
    "ytics = [\"2C\", \"2B\", \"2A\", \"3\"]\n",
    "# Set the palette using the name of a palette:\n",
    "sns.heatmap(heat, annot=True, xticklabels=xtics, yticklabels=ytics, cmap=\"Blues\", cbar=False)\n",
    "# sns.heatmap(heat, annot=True, cmap=\"Blues\", cbar=False)\n",
    "#*以下2行がポイント*  X,Y軸ラベルを追加\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "#グラフをはみ出さないようにして画面に出力\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"nn-heatmap_crossvalid.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプルごとに代償動作の度合いをセッティングしてみる\n",
    "preds = np.array(outputs.cpu())\n",
    "ans = np.array(labels_true.cpu())\n",
    "plt.scatter(preds, ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l_attention_weights[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 25\n",
    "\n",
    "print(f\"predicts are {preds[index]}, labels {labels_true[index]}\")\n",
    "\n",
    "RElbow2RWrist_weight = np.array(l_attention_weights[0].cpu()[index, 0, 1:]).reshape(1, -1)\n",
    "RShoulder2RElbow_weight = np.array(l_attention_weights[1].cpu()[index, 0, 1:]).reshape(1, -1)\n",
    "Pelvis2RShoulder_weight = np.array(l_attention_weights[2].cpu()[index, 0, 1:]).reshape(1, -1)\n",
    "Pelvis2RNeck_weight = np.array(l_attention_weights[3].cpu()[index, 0, 1:]).reshape(1, -1)\n",
    "Nect2RShoulder_weight = np.array(l_attention_weights[4].cpu()[index, 0, 1:]).reshape(1, -1)\n",
    "Neck2LShoulder_weight = np.array(l_attention_weights[5].cpu()[index, 0, 1:]).reshape(1, -1)\n",
    "\n",
    "weights = np.concatenate([RElbow2RWrist_weight, RShoulder2RElbow_weight, Pelvis2RShoulder_weight, Pelvis2RNeck_weight,\n",
    "                         Nect2RShoulder_weight, Neck2LShoulder_weight])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "# Set the palette using the name of a palette:\n",
    "# sns.heatmap(RElbow2RWrist_weight, cmap='OrRd')\n",
    "# sns.heatmap(RShoulder2RElbow_weight, cmap='OrRd')\n",
    "# sns.heatmap(Pelvis2RShoulder_weight, cmap='OrRd')\n",
    "# sns.heatmap(Pelvis2RNeck_weight, cmap='OrRd')\n",
    "# sns.heatmap(Nect2RShoulder_weight, cmap='OrRd')\n",
    "# sns.heatmap(Neck2LShoulder_weight, cmap='OrRd')\n",
    "\n",
    "sns.heatmap(weights, cmap='OrRd')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "extract_data = np.array(X[19, 1:, 2]).flatten()\n",
    "print(extract_data.shape)\n",
    "\n",
    "\n",
    "time = np.linspace(0, 100, 100)\n",
    "fig, ax = plt.subplots(2, 1, gridspec_kw={'height_ratios': [6, 1]}, figsize=(15,10))\n",
    "# Set the palette using the name of a palette:\n",
    "sns.lineplot(time, extract_data, ax=ax[0])\n",
    "sns.heatmap(extract_weight, cmap='OrRd', cbar=False, ax=ax[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "data = np.array(attn_weights1[:, 0, :])\n",
    "print(data.shape)\n",
    "data = data.reshape(attn_weights1.shape[0], -1)\n",
    "\n",
    "# print(outputs)\n",
    "print(data.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30, 20))\n",
    "# Set the palette using the name of a palette:\n",
    "sns.heatmap(data, cmap='OrRd')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data[0])):\n",
    "    print(data[0,i], end=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_trained.net_Attention_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelの読み込みだけ\n",
    "# 保存したモデルパラメータの読み込み\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net_trained = TransformerClassification().to(device)\n",
    "\n",
    "net_trained.load_state_dict(torch.load('highacc2.pth'))\n",
    "# print('読み込み後のモデル:\\n', model2.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(net_trained.state_dict(), 'highacc3.pth')\n",
    "\n",
    "# 新しいモデル\n",
    "model2 = TransformerClassification().to(device)\n",
    "print('新しいモデル:\\n', model2.state_dict())\n",
    "\n",
    "# 保存したモデルパラメータの読み込み\n",
    "model2.load_state_dict(torch.load('highacc3.pth'))\n",
    "print('読み込み後のモデル:\\n', model2.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attention weight の保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape\n",
    "print(data)\n",
    "print(data.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 同時に入れるデータも作成する\n",
    "# これは時系列スタンプ．0〜100がサンプル数だけ作られる\n",
    "stamp_row = [i%data.shape[1] for i in range(data.shape[0] * data.shape[1])]\n",
    "\n",
    "# いつ撮影されたのかのスタンプ\n",
    "l_date_name = np.array(df_annotate['date'].unique())\n",
    "date_row = []\n",
    "for item in l_date_name:\n",
    "    for i in range(data.shape[1]):\n",
    "        date_row.append(item)\n",
    "        \n",
    "# アテンションウエイト\n",
    "attn_weight = data.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attn = pd.DataFrame()\n",
    "df_attn['date'] = date_row\n",
    "df_attn[\"stamp\"] = stamp_row\n",
    "df_attn[\"attn_weight\"] = attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attn.to_excel('/Users/kento/kuhp/experiment/attention_weights.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_attn['date'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "timeseries_transformer_classification.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/keras-team/keras-io/blob/master/examples/timeseries/ipynb/timeseries_classification_transformer.ipynb",
     "timestamp": 1636355482830
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
